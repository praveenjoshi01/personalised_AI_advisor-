{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T21:33:10.301595Z",
     "start_time": "2019-05-20T21:33:08.538332Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import abc\n",
    "import re\n",
    "import nltk\n",
    "from sklearn import model_selection \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T21:33:10.333548Z",
     "start_time": "2019-05-20T21:33:10.303590Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T21:33:10.338517Z",
     "start_time": "2019-05-20T21:33:10.334507Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T21:33:10.401369Z",
     "start_time": "2019-05-20T21:33:10.341491Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "import datetime\n",
    "import missingno as mn\n",
    "import pylab as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T21:33:10.408356Z",
     "start_time": "2019-05-20T21:33:10.403324Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T21:33:10.420285Z",
     "start_time": "2019-05-20T21:33:10.409308Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:42.639782Z",
     "start_time": "2019-05-20T22:11:42.559958Z"
    }
   },
   "outputs": [],
   "source": [
    "featureDF_nw = pd.read_csv('nw_feature_woText_MSFT.csv')\n",
    "featureDF_tw = pd.read_csv('tw_feature_woText_MSFT.csv')\n",
    "targetDF_ = pd.read_csv('FIwithTargetwithFTT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:42.864510Z",
     "start_time": "2019-05-20T22:11:42.860514Z"
    }
   },
   "outputs": [],
   "source": [
    "def createComparableDataset(_df):\n",
    "    _df.date = pd.to_datetime(_df.date)\n",
    "    _df = _df[_df['date'] >= pd.to_datetime('2015/05/04')]\n",
    "    _df = _df[_df['date'] <= pd.to_datetime('2019/03/28')]\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:43.109518Z",
     "start_time": "2019-05-20T22:11:43.104503Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepareDataSet(_df):\n",
    "    _df.date = pd.to_datetime(_df.date)\n",
    "    train = _df[_df['date'] < pd.to_datetime('2019/01/01')]\n",
    "    test = _df[_df['date'] >= pd.to_datetime('2019/01/01')]\n",
    "    \n",
    "    train, train_y = getFeaturesAndTargetRemoveDate(train)\n",
    "    test, test_y = getFeaturesAndTargetRemoveDate(test)\n",
    "    return train, train_y,test, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:43.418797Z",
     "start_time": "2019-05-20T22:11:43.414833Z"
    }
   },
   "outputs": [],
   "source": [
    "def createTrainingAndTestingSet(_df):\n",
    "    train = _df[_df['date'] < pd.to_datetime('2019/01/01')]\n",
    "    test = _df[_df['date'] >= pd.to_datetime('2019/01/01')]\n",
    "    return train ,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:43.866992Z",
     "start_time": "2019-05-20T22:11:43.862987Z"
    }
   },
   "outputs": [],
   "source": [
    "def getStartEndDateShape(dfz):\n",
    "    startDate = str(min(dfz['date']))\n",
    "    endDate = str(max(dfz['date']))\n",
    "    print('startDate', startDate)\n",
    "    print('endDate', endDate)\n",
    "    print('shape', dfz.shape)\n",
    "    print('---------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:44.148627Z",
     "start_time": "2019-05-20T22:11:44.144610Z"
    }
   },
   "outputs": [],
   "source": [
    "def getFeaturesAndTargetRemoveDate(_df):\n",
    "    try:\n",
    "        train_y = _df['target']\n",
    "        train = _df.drop(['date','target'],axis =1)\n",
    "    except:\n",
    "        train_y = _df['Target']\n",
    "        train = _df.drop(['date','Target'],axis =1)        \n",
    "    return train, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:44.475353Z",
     "start_time": "2019-05-20T22:11:44.467400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2004/07/21\n",
      "endDate 2019/03/28\n",
      "shape (4020, 24)\n",
      "---------------------------\n",
      "startDate 2015/05/04\n",
      "endDate 2019/04/04\n",
      "shape (1426, 25)\n",
      "---------------------------\n",
      "startDate 2003/04/30\n",
      "endDate 2019/04/03\n",
      "shape (4010, 32)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "getStartEndDateShape(featureDF_nw)\n",
    "getStartEndDateShape(featureDF_tw)\n",
    "getStartEndDateShape(targetDF_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:44.741142Z",
     "start_time": "2019-05-20T22:11:44.734157Z"
    }
   },
   "outputs": [],
   "source": [
    "targtWithDateForJoin =pd.DataFrame()\n",
    "targtWithDateForJoin['date'] = targetDF_.date.tolist()\n",
    "targtWithDateForJoin['target'] = targetDF_.Target.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T05:59:14.290447Z",
     "start_time": "2019-04-11T05:59:14.286449Z"
    }
   },
   "source": [
    "# Formulation of overall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:45.319062Z",
     "start_time": "2019-05-20T22:11:45.309099Z"
    }
   },
   "outputs": [],
   "source": [
    "featureDF_nw.date = pd.to_datetime(featureDF_nw.date)\n",
    "targtWithDateForJoin.date = pd.to_datetime(targtWithDateForJoin.date)\n",
    "featureDF_tw.date = pd.to_datetime(featureDF_tw.date)\n",
    "targetDF_.date = pd.to_datetime(targetDF_.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:45.931561Z",
     "start_time": "2019-05-20T22:11:45.642139Z"
    }
   },
   "outputs": [],
   "source": [
    "featureDF_nw= createComparableDataset(featureDF_nw)\n",
    "targtWithDateForJoin=createComparableDataset(targtWithDateForJoin)\n",
    "featureDF_tw=createComparableDataset(featureDF_tw)\n",
    "targetDF_=createComparableDataset(targetDF_)\n",
    "targetDF_.rename(columns={'Target':'target'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:45.974053Z",
     "start_time": "2019-05-20T22:11:45.955104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (1027, 24)\n",
      "---------------------------\n",
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (1419, 25)\n",
      "---------------------------\n",
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (983, 32)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "getStartEndDateShape(featureDF_nw)\n",
    "getStartEndDateShape(featureDF_tw)\n",
    "getStartEndDateShape(targetDF_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:46.220289Z",
     "start_time": "2019-05-20T22:11:46.209292Z"
    }
   },
   "outputs": [],
   "source": [
    "result =pd.merge( targetDF_,featureDF_nw, how='outer', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:46.499248Z",
     "start_time": "2019-05-20T22:11:46.489282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (1171, 55)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "getStartEndDateShape(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:46.811254Z",
     "start_time": "2019-05-20T22:11:46.798249Z"
    }
   },
   "outputs": [],
   "source": [
    "result =pd.merge( result,featureDF_tw, how='outer', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:47.102053Z",
     "start_time": "2019-05-20T22:11:47.092078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (1423, 79)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "getStartEndDateShape(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:47.374757Z",
     "start_time": "2019-05-20T22:11:47.367784Z"
    }
   },
   "outputs": [],
   "source": [
    "result.sort_values(by='date',inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:47.703040Z",
     "start_time": "2019-05-20T22:11:47.697017Z"
    }
   },
   "outputs": [],
   "source": [
    "result = result.ffill()\n",
    "result.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:48.031714Z",
     "start_time": "2019-05-20T22:11:48.008754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>sma2</th>\n",
       "      <th>sma2_1</th>\n",
       "      <th>sma2_increment</th>\n",
       "      <th>sma2_1_increment</th>\n",
       "      <th>...</th>\n",
       "      <th>disgust_y</th>\n",
       "      <th>anticipation_y</th>\n",
       "      <th>fear_y</th>\n",
       "      <th>trust_y</th>\n",
       "      <th>tb_polarity_y</th>\n",
       "      <th>tb_subjectivity_y</th>\n",
       "      <th>hiv4_positive_y</th>\n",
       "      <th>hiv4_negative_y</th>\n",
       "      <th>hiv4_polarity_y</th>\n",
       "      <th>hiv4_subjectivity_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.369999</td>\n",
       "      <td>48.869999</td>\n",
       "      <td>48.180000</td>\n",
       "      <td>48.240002</td>\n",
       "      <td>44.072769</td>\n",
       "      <td>34039500.0</td>\n",
       "      <td>48.475001</td>\n",
       "      <td>48.640002</td>\n",
       "      <td>-0.165001</td>\n",
       "      <td>-0.069999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>2.383333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.333331</td>\n",
       "      <td>0.589387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.820000</td>\n",
       "      <td>48.160000</td>\n",
       "      <td>47.310001</td>\n",
       "      <td>47.599998</td>\n",
       "      <td>43.488049</td>\n",
       "      <td>50369200.0</td>\n",
       "      <td>48.095000</td>\n",
       "      <td>48.475001</td>\n",
       "      <td>-0.380001</td>\n",
       "      <td>-0.165001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.666666</td>\n",
       "      <td>0.326007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.570000</td>\n",
       "      <td>47.770000</td>\n",
       "      <td>46.020000</td>\n",
       "      <td>46.279999</td>\n",
       "      <td>42.282074</td>\n",
       "      <td>52433000.0</td>\n",
       "      <td>47.695000</td>\n",
       "      <td>48.095000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.380001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.250947</td>\n",
       "      <td>7.171212</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.999995</td>\n",
       "      <td>3.123412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46.270000</td>\n",
       "      <td>47.090000</td>\n",
       "      <td>46.160000</td>\n",
       "      <td>46.700001</td>\n",
       "      <td>42.665791</td>\n",
       "      <td>32971700.0</td>\n",
       "      <td>46.920000</td>\n",
       "      <td>47.695000</td>\n",
       "      <td>-0.775000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.322159</td>\n",
       "      <td>3.534091</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-3.999997</td>\n",
       "      <td>1.258474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.549999</td>\n",
       "      <td>47.980000</td>\n",
       "      <td>47.520000</td>\n",
       "      <td>47.750000</td>\n",
       "      <td>43.625092</td>\n",
       "      <td>35364900.0</td>\n",
       "      <td>46.909999</td>\n",
       "      <td>46.920000</td>\n",
       "      <td>-0.010001</td>\n",
       "      <td>-0.775000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.333332</td>\n",
       "      <td>1.455759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open       High        Low      Close  Adj Close      Volume  \\\n",
       "0  48.369999  48.869999  48.180000  48.240002  44.072769  34039500.0   \n",
       "1  47.820000  48.160000  47.310001  47.599998  43.488049  50369200.0   \n",
       "2  47.570000  47.770000  46.020000  46.279999  42.282074  52433000.0   \n",
       "3  46.270000  47.090000  46.160000  46.700001  42.665791  32971700.0   \n",
       "4  47.549999  47.980000  47.520000  47.750000  43.625092  35364900.0   \n",
       "\n",
       "        sma2     sma2_1  sma2_increment  sma2_1_increment  ...  disgust_y  \\\n",
       "0  48.475001  48.640002       -0.165001         -0.069999  ...        0.0   \n",
       "1  48.095000  48.475001       -0.380001         -0.165001  ...        0.0   \n",
       "2  47.695000  48.095000       -0.400000         -0.380001  ...        0.0   \n",
       "3  46.920000  47.695000       -0.775000         -0.400000  ...        0.0   \n",
       "4  46.909999  46.920000       -0.010001         -0.775000  ...        0.0   \n",
       "\n",
       "   anticipation_y  fear_y   trust_y  tb_polarity_y  tb_subjectivity_y  \\\n",
       "0        0.002845     0.0  0.000000       2.400000           2.383333   \n",
       "1        0.003759     0.0  0.000000       0.000000           0.300000   \n",
       "2        0.000522     0.0  0.000000       1.250947           7.171212   \n",
       "3        0.000573     0.0  0.000573       0.322159           3.534091   \n",
       "4        0.000548     0.0  0.000000       0.325000           0.962500   \n",
       "\n",
       "   hiv4_positive_y  hiv4_negative_y  hiv4_polarity_y  hiv4_subjectivity_y  \n",
       "0              5.0              2.0         2.333331             0.589387  \n",
       "1              3.0              2.0         0.666666             0.326007  \n",
       "2             19.0              5.0         7.999995             3.123412  \n",
       "3              2.0              7.0        -3.999997             1.258474  \n",
       "4             11.0              6.0         2.333332             1.455759  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:48.335714Z",
     "start_time": "2019-05-20T22:11:48.309784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>sma2</th>\n",
       "      <th>sma2_1</th>\n",
       "      <th>sma2_increment</th>\n",
       "      <th>sma2_1_increment</th>\n",
       "      <th>...</th>\n",
       "      <th>disgust_y</th>\n",
       "      <th>anticipation_y</th>\n",
       "      <th>fear_y</th>\n",
       "      <th>trust_y</th>\n",
       "      <th>tb_polarity_y</th>\n",
       "      <th>tb_subjectivity_y</th>\n",
       "      <th>hiv4_positive_y</th>\n",
       "      <th>hiv4_negative_y</th>\n",
       "      <th>hiv4_polarity_y</th>\n",
       "      <th>hiv4_subjectivity_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>119.500000</td>\n",
       "      <td>119.589996</td>\n",
       "      <td>117.040001</td>\n",
       "      <td>117.050003</td>\n",
       "      <td>117.050003</td>\n",
       "      <td>33624500.0</td>\n",
       "      <td>118.319999</td>\n",
       "      <td>117.264999</td>\n",
       "      <td>1.055001</td>\n",
       "      <td>-0.474998</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>54.011847</td>\n",
       "      <td>133.276238</td>\n",
       "      <td>565.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>99.701533</td>\n",
       "      <td>75.265758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>116.559998</td>\n",
       "      <td>118.010002</td>\n",
       "      <td>116.320000</td>\n",
       "      <td>117.660004</td>\n",
       "      <td>117.660004</td>\n",
       "      <td>27067100.0</td>\n",
       "      <td>118.029999</td>\n",
       "      <td>118.319999</td>\n",
       "      <td>-0.290000</td>\n",
       "      <td>1.055001</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>21.654621</td>\n",
       "      <td>77.138890</td>\n",
       "      <td>361.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>62.624211</td>\n",
       "      <td>41.045214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>118.620003</td>\n",
       "      <td>118.709999</td>\n",
       "      <td>116.849998</td>\n",
       "      <td>117.910004</td>\n",
       "      <td>117.910004</td>\n",
       "      <td>26097700.0</td>\n",
       "      <td>117.590000</td>\n",
       "      <td>118.029999</td>\n",
       "      <td>-0.439999</td>\n",
       "      <td>-0.290000</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>98.739210</td>\n",
       "      <td>286.580670</td>\n",
       "      <td>1363.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>248.350599</td>\n",
       "      <td>147.048014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>117.879997</td>\n",
       "      <td>118.209999</td>\n",
       "      <td>115.519997</td>\n",
       "      <td>116.769997</td>\n",
       "      <td>116.769997</td>\n",
       "      <td>22733400.0</td>\n",
       "      <td>118.250000</td>\n",
       "      <td>117.590000</td>\n",
       "      <td>0.659999</td>\n",
       "      <td>-0.439999</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>110.125758</td>\n",
       "      <td>283.081651</td>\n",
       "      <td>1428.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>244.546184</td>\n",
       "      <td>132.279292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>117.440002</td>\n",
       "      <td>117.580002</td>\n",
       "      <td>116.129997</td>\n",
       "      <td>116.930000</td>\n",
       "      <td>116.930000</td>\n",
       "      <td>18334800.0</td>\n",
       "      <td>117.659999</td>\n",
       "      <td>118.250000</td>\n",
       "      <td>-0.590001</td>\n",
       "      <td>0.659999</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>101.462943</td>\n",
       "      <td>299.296855</td>\n",
       "      <td>1406.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>267.852539</td>\n",
       "      <td>148.186288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open        High         Low       Close   Adj Close      Volume  \\\n",
       "1092  119.500000  119.589996  117.040001  117.050003  117.050003  33624500.0   \n",
       "979   116.559998  118.010002  116.320000  117.660004  117.660004  27067100.0   \n",
       "980   118.620003  118.709999  116.849998  117.910004  117.910004  26097700.0   \n",
       "981   117.879997  118.209999  115.519997  116.769997  116.769997  22733400.0   \n",
       "982   117.440002  117.580002  116.129997  116.930000  116.930000  18334800.0   \n",
       "\n",
       "            sma2      sma2_1  sma2_increment  sma2_1_increment  ...  \\\n",
       "1092  118.319999  117.264999        1.055001         -0.474998  ...   \n",
       "979   118.029999  118.319999       -0.290000          1.055001  ...   \n",
       "980   117.590000  118.029999       -0.439999         -0.290000  ...   \n",
       "981   118.250000  117.590000        0.659999         -0.439999  ...   \n",
       "982   117.659999  118.250000       -0.590001          0.659999  ...   \n",
       "\n",
       "      disgust_y  anticipation_y    fear_y   trust_y  tb_polarity_y  \\\n",
       "1092       12.0        0.001662  0.000345  0.001552      54.011847   \n",
       "979         4.0        0.001268  0.000215  0.000933      21.654621   \n",
       "980        19.0        0.001762  0.000370  0.001193      98.739210   \n",
       "981        17.0        0.001513  0.000332  0.001175     110.125758   \n",
       "982        27.0        0.001972  0.000315  0.001134     101.462943   \n",
       "\n",
       "      tb_subjectivity_y  hiv4_positive_y  hiv4_negative_y  hiv4_polarity_y  \\\n",
       "1092         133.276238            565.0            273.0        99.701533   \n",
       "979           77.138890            361.0            156.0        62.624211   \n",
       "980          286.580670           1363.0            625.0       248.350599   \n",
       "981          283.081651           1428.0            565.0       244.546184   \n",
       "982          299.296855           1406.0            590.0       267.852539   \n",
       "\n",
       "      hiv4_subjectivity_y  \n",
       "1092            75.265758  \n",
       "979             41.045214  \n",
       "980            147.048014  \n",
       "981            132.279292  \n",
       "982            148.186288  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:48.774813Z",
     "start_time": "2019-05-20T22:11:48.611252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save data with without scaling\n",
    "result.to_csv('OverAllDataset_wo_scaling.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:48.953433Z",
     "start_time": "2019-05-20T22:11:48.943466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (1423, 79)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "getStartEndDateShape(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:49.300852Z",
     "start_time": "2019-05-20T22:11:49.291850Z"
    }
   },
   "outputs": [],
   "source": [
    "date= result.date.tolist()\n",
    "target = result.target.tolist()\n",
    "result.drop(['date','target'],axis =1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:49.628947Z",
     "start_time": "2019-05-20T22:11:49.619970Z"
    }
   },
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "result = pd.DataFrame(scale.fit_transform(result.values), columns=result.columns, index=result.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:49.922195Z",
     "start_time": "2019-05-20T22:11:49.917176Z"
    }
   },
   "outputs": [],
   "source": [
    "result['date'] = date\n",
    "result['Target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:50.459633Z",
     "start_time": "2019-05-20T22:11:50.255108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save data with without scaling                   \n",
    "result.to_csv('OverAllDataset_with_scaling.csv',index= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:50.529200Z",
     "start_time": "2019-05-20T22:11:50.518229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (1423, 79)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "getStartEndDateShape(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:51.077934Z",
     "start_time": "2019-05-20T22:11:51.069957Z"
    }
   },
   "outputs": [],
   "source": [
    "train,test = createTrainingAndTestingSet(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:51.412042Z",
     "start_time": "2019-05-20T22:11:51.401087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2018-12-31 00:00:00\n",
      "shape (1336, 79)\n",
      "---------------------------\n",
      "startDate 2019-01-01 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (87, 79)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "getStartEndDateShape(train)\n",
    "getStartEndDateShape(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:51.952208Z",
     "start_time": "2019-05-20T22:11:51.739647Z"
    }
   },
   "outputs": [],
   "source": [
    "train.to_csv('train_tw_nw_fi.csv',index=False)\n",
    "test.to_csv('test_tw_nw_fi.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:52.222954Z",
     "start_time": "2019-05-20T22:11:52.184022Z"
    }
   },
   "outputs": [],
   "source": [
    "nonScaledDataset = pd.read_csv('OverAllDataset_wo_scaling.csv')\n",
    "nonScaledDataset.date = pd.to_datetime(nonScaledDataset.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:53.593321Z",
     "start_time": "2019-05-20T22:11:53.586336Z"
    }
   },
   "outputs": [],
   "source": [
    "train_en, test_en = createTrainingAndTestingSet(nonScaledDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:54.218221Z",
     "start_time": "2019-05-20T22:11:54.206250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2018-12-31 00:00:00\n",
      "shape (1336, 79)\n",
      "---------------------------\n",
      "startDate 2019-01-01 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (87, 79)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "getStartEndDateShape(train_en)\n",
    "getStartEndDateShape(test_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:55.002772Z",
     "start_time": "2019-05-20T22:11:54.789190Z"
    }
   },
   "outputs": [],
   "source": [
    "train_en.to_csv('train_ns_tw_nw_fi.csv',index= False)\n",
    "test_en.to_csv('test_ns_tw_nw_fi.csv',index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T02:52:08.923857Z",
     "start_time": "2019-04-10T02:52:08.920847Z"
    }
   },
   "source": [
    "# NEWS RandomForest- NonScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:15:58.692492Z",
     "start_time": "2019-05-20T22:15:58.680472Z"
    }
   },
   "outputs": [],
   "source": [
    "featureDF_nw.date = pd.to_datetime(featureDF_nw.date)\n",
    "targtWithDateForJoin.date = pd.to_datetime(targtWithDateForJoin.date)\n",
    "result =pd.merge(featureDF_nw, targtWithDateForJoin, how='left', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:15:59.131410Z",
     "start_time": "2019-05-20T22:15:59.124430Z"
    }
   },
   "outputs": [],
   "source": [
    "result.sort_values(by='date',inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:15:59.418815Z",
     "start_time": "2019-05-20T22:15:59.414799Z"
    }
   },
   "outputs": [],
   "source": [
    "result['target'] = result['target'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:15:59.707930Z",
     "start_time": "2019-05-20T22:15:59.704939Z"
    }
   },
   "outputs": [],
   "source": [
    "_ens_News_NonScaled_FE = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:15:59.983456Z",
     "start_time": "2019-05-20T22:15:59.973513Z"
    }
   },
   "outputs": [],
   "source": [
    "#train and test split\n",
    "train_nw, test_nw = createTrainingAndTestingSet(result)\n",
    "\n",
    "train_nw_y = train_nw['target']\n",
    "test_nw_y = test_nw['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:16:00.307777Z",
     "start_time": "2019-05-20T22:16:00.298802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2018-12-30 00:00:00\n",
      "shape (961, 25)\n",
      "---------------------------\n",
      "startDate 2019-01-03 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (66, 25)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "getStartEndDateShape(train_nw)\n",
    "getStartEndDateShape(test_nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:16:02.101089Z",
     "start_time": "2019-05-20T22:16:01.602860Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praveen ji\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_nw.drop(['date','target'],axis=1,inplace=True)\n",
    "test_nw.drop(['date','target'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:16:02.364848Z",
     "start_time": "2019-05-20T22:16:02.358867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_nw (961, 23) test_nw (66, 23) len(train_nw_y) 961 len(test_nw_y) 66\n"
     ]
    }
   ],
   "source": [
    "print('train_nw',train_nw.shape,'test_nw',test_nw.shape,'len(train_nw_y)',len(train_nw_y),'len(test_nw_y)',len(test_nw_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:16:04.048460Z",
     "start_time": "2019-05-20T22:16:04.044490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Naive Bayes for Baseline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "def NaiveBayes(train_features , train_labels, test_features,test_labels):\n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(train_features, train_labels).predict(test_features)\n",
    "    print('accuracy_score',accuracy_score(y_pred, test_labels))\n",
    "    print('classification_score\\n',classification_report(y_pred, test_labels))\n",
    "    return gnb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:16:04.436087Z",
     "start_time": "2019-05-20T22:16:04.431101Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotImportanceOfFeature(features,clf):\n",
    "    importances = clf.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:16:04.946177Z",
     "start_time": "2019-05-20T22:16:04.940210Z"
    }
   },
   "outputs": [],
   "source": [
    "def corelationPlot (df_):\n",
    "    corr = df_.corr()\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    fig.colorbar(cax)\n",
    "    ticks = np.arange(0,len(df_.columns),1)\n",
    "    ax.set_xticks(ticks)\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_xticklabels(df_.columns)\n",
    "    ax.set_yticklabels(df_.columns)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:16:05.212322Z",
     "start_time": "2019-05-20T22:16:05.204371Z"
    }
   },
   "outputs": [],
   "source": [
    "def vif_RemoveHiglyCorelatedFeatures(_df,_dfTest,corPlot = False):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF Factor\"] = [variance_inflation_factor(_df.values, i) for i in range(_df.values.shape[1])]\n",
    "    vif.round(1)\n",
    "    while max(vif[\"VIF Factor\"])>10:\n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"VIF Factor\"] = [variance_inflation_factor(_df.values, i) for i in range(_df.values.shape[1])]\n",
    "        vif.round(1)\n",
    "        dropCol = [_df.columns[np.argmax(vif[\"VIF Factor\"])]]\n",
    "        _df.drop(columns=dropCol, inplace=True)\n",
    "        _dfTest.drop(columns=dropCol, inplace=True)\n",
    "        if corPlot:\n",
    "            corelationPlot (_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:16:05.513259Z",
     "start_time": "2019-05-20T22:16:05.505282Z"
    }
   },
   "outputs": [],
   "source": [
    "def cCMatrixAccRF(train_features , train_labels, test_features,test_labels, features, dispImpOfFeature = False):\n",
    "#         train_features, test_features, train_labels, test_labels= model_selection.train_test_split( featureDF , targetValue, test_size=0.1, random_state=0)\n",
    "\n",
    "        rf_model =RandomForestClassifier(n_estimators=5000, max_depth=4,random_state=0)\n",
    "        rf_model.fit(train_features, train_labels)\n",
    "        y_predicted=rf_model.predict(test_features)\n",
    "        print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++' )\n",
    "        \n",
    "        if dispImpOfFeature:\n",
    "            plotImportanceOfFeature(features, rf_model)\n",
    "        print('confusion_matrix',confusion_matrix(y_true=test_labels, y_pred=y_predicted))\n",
    "        print('number of datapoints in Up Class', test_labels.value_counts()[1])\n",
    "        print('number of datapoints in Down Class', test_labels.value_counts()[0])\n",
    "        print('accuracy_score',accuracy_score(y_predicted, test_labels))\n",
    "        print('classification_score\\n',classification_report(y_predicted, test_labels))\n",
    "        print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "        return rf_model\n",
    "#         return rf_model.predict(train_features),rf_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:16:09.703712Z",
     "start_time": "2019-05-20T22:16:05.811825Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praveen ji\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:56: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "vif_RemoveHiglyCorelatedFeatures(train_nw,test_nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:12:21.771665Z",
     "start_time": "2019-05-20T22:12:21.766695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_nw (961, 16) test_nw (66, 16) len(train_nw_y) 961 len(test_nw_y) 66\n"
     ]
    }
   ],
   "source": [
    "print('train_nw',train_nw.shape,'test_nw',test_nw.shape,'len(train_nw_y)',len(train_nw_y),'len(test_nw_y)',len(test_nw_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:12:46.020790Z",
     "start_time": "2019-05-20T22:12:46.014806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lm_positive', 'lm_negative', 'lm_polarity', 'passive', 'weak',\n",
       "       'strong', 'anger', 'joy', 'suprise', 'sadness', 'disgust',\n",
       "       'anticipation', 'fear', 'trust', 'tb_polarity', 'hiv4_polarity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:01:26.739086Z",
     "start_time": "2019-05-20T22:01:26.728115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.6212121212121212\n",
      "classification_score\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.67      0.39        12\n",
      "         1.0       0.89      0.61      0.73        54\n",
      "\n",
      "   micro avg       0.62      0.62      0.62        66\n",
      "   macro avg       0.58      0.64      0.56        66\n",
      "weighted avg       0.78      0.62      0.66        66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_model = NaiveBayes(train_nw , train_nw_y, test_nw,test_nw_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:31:15.960457Z",
     "start_time": "2019-05-20T22:31:15.946495Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'news_model.sav'\n",
    "pickle.dump(news_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Indicator RandomForest- Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:15:28.967324Z",
     "start_time": "2019-05-20T22:15:28.962352Z"
    }
   },
   "outputs": [],
   "source": [
    "_ens_FI_Scaled_FE =fi_df_copy = targetDF_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:13:04.563944Z",
     "start_time": "2019-05-20T22:13:04.042350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2018-12-31 00:00:00\n",
      "shape (923, 32)\n",
      "---------------------------\n",
      "startDate 2019-01-02 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (60, 32)\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praveen ji\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "train_fi, test_fi= createTrainingAndTestingSet(fi_df_copy)\n",
    "\n",
    "train_fi_y = train_fi['target']\n",
    "test_fi_y = test_fi['target']\n",
    "\n",
    "getStartEndDateShape(train_fi)\n",
    "getStartEndDateShape(test_fi)\n",
    "\n",
    "train_fi.drop(['date','target'],axis=1,inplace=True)\n",
    "test_fi.drop(['date','target'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:13:04.573930Z",
     "start_time": "2019-05-20T22:13:04.565937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_fi (923, 30) test_fi (60, 30) len(train_fi_y) 923 len(test_fi_y) 60\n"
     ]
    }
   ],
   "source": [
    "print('train_fi',train_fi.shape,'test_fi',test_fi.shape,'len(train_fi_y)',len(train_fi_y),'len(test_fi_y)',len(test_fi_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:13:16.029877Z",
     "start_time": "2019-05-20T22:13:05.175908Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praveen ji\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:181: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "C:\\Users\\praveen ji\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:56: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "vif_RemoveHiglyCorelatedFeatures(train_fi,test_fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:13:16.038865Z",
     "start_time": "2019-05-20T22:13:16.031888Z"
    }
   },
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "train_fi = pd.DataFrame(scale.fit_transform(train_fi.values), columns=train_fi.columns, index=train_fi.index)\n",
    "scale = StandardScaler()\n",
    "test_fi = pd.DataFrame(scale.fit_transform(test_fi.values), columns=test_fi.columns, index=test_fi.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:13:16.048837Z",
     "start_time": "2019-05-20T22:13:16.040844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_fi (923, 9) test_fi (60, 9) len(train_fi_y) 923 len(test_fi_y) 60\n"
     ]
    }
   ],
   "source": [
    "print('train_fi',train_fi.shape,'test_fi',test_fi.shape,'len(train_fi_y)',len(train_fi_y),'len(test_fi_y)',len(test_fi_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:13:33.529759Z",
     "start_time": "2019-05-20T22:13:33.523802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Volume', 'sma2_increment', 'sma2_1_increment', 'vol_increment',\n",
       "       'vol_rel_increment', 'open_incr', 'MACD', 'fft', 'absolute'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T21:33:16.155292Z",
     "start_time": "2019-05-20T21:33:16.136343Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_fi (923, 9) test_fi (60, 9) len(train_fi_y) 923 len(test_fi_y) 60\n"
     ]
    }
   ],
   "source": [
    "vif_RemoveHiglyCorelatedFeatures(train_fi,test_fi)\n",
    "print('train_fi',train_fi.shape,'test_fi',test_fi.shape,'len(train_fi_y)',len(train_fi_y),'len(test_fi_y)',len(test_fi_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:32:43.889286Z",
     "start_time": "2019-05-20T22:32:37.803556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "confusion_matrix [[22  3]\n",
      " [10 25]]\n",
      "number of datapoints in Up Class 35\n",
      "number of datapoints in Down Class 25\n",
      "accuracy_score 0.7833333333333333\n",
      "classification_score\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.69      0.77        32\n",
      "           1       0.71      0.89      0.79        28\n",
      "\n",
      "   micro avg       0.78      0.78      0.78        60\n",
      "   macro avg       0.80      0.79      0.78        60\n",
      "weighted avg       0.80      0.78      0.78        60\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "features = list(train_fi.columns)\n",
    "fi_model = cCMatrixAccRF(train_fi , train_fi_y, test_fi,test_fi_y, features, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:32:54.412067Z",
     "start_time": "2019-05-20T22:32:54.222387Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'fi_model.sav'\n",
    "pickle.dump(fi_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Model -BERT - Dense Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T21:33:23.544292Z",
     "start_time": "2019-05-20T21:33:22.278926Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T21:33:23.597918Z",
     "start_time": "2019-05-20T21:33:23.546051Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T21:33:23.604882Z",
     "start_time": "2019-05-20T21:33:23.599870Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Bidirectional , GRU , RepeatVector\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "import pandas\n",
    "from keras.models import load_model\n",
    "from os.path import isfile\n",
    "import os\n",
    "from keras.optimizers import Adam\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T23:22:51.067866Z",
     "start_time": "2019-05-20T23:22:51.061883Z"
    }
   },
   "outputs": [],
   "source": [
    "def DenseNetwork(featureLength):\n",
    "    #defifne a sequentail Model\n",
    "    model = Sequential()\n",
    "\n",
    "    #Hidden Layer-1\n",
    "    model.add(Dense(100,activation='relu',input_dim=featureLength,kernel_regularizer=l2(0.60)))\n",
    "    model.add(Dropout(0.3, noise_shape=None, seed=155))\n",
    "\n",
    "    #Hidden Layer-2\n",
    "    model.add(Dense(100,activation = 'relu',kernel_regularizer=l2(0.60)))\n",
    "    model.add(Dropout(0.3, noise_shape=None, seed=155))\n",
    "\n",
    "    #Output layer\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:26:44.751887Z",
     "start_time": "2019-05-20T22:26:44.744907Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluateModelAndTestAccuracy(model,trainX, trainY, testX, testY,n_epoch):\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(trainX, trainY, epochs=n_epoch, validation_split=0.1,batch_size=128, verbose=1,)\n",
    "    \n",
    "    results = model.evaluate(testX, testY)\n",
    "    \n",
    "    y_pred = model.predict_classes(testX)\n",
    "    print(\"Evaluation on test data: Loss - {0},  Acurracy - {1}\".format(results[0],str(results[1]*100)))\n",
    "    print('classification_score\\n',classification_report(y_pred, testY))\n",
    "    historyDict= {}\n",
    "    historyDict['history'] = history\n",
    "    historyDict['model'] = model\n",
    "    return historyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:24:11.589738Z",
     "start_time": "2019-05-20T22:24:11.583752Z"
    }
   },
   "outputs": [],
   "source": [
    "def vizModelMetric(historyDict,n_epoch):\n",
    "    history = historyDict['history']\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, n_epoch), history.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, n_epoch), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, n_epoch), history.history[\"acc\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, n_epoch), history.history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    return historyDict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:25:17.220349Z",
     "start_time": "2019-05-20T22:25:16.801334Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_featureDF_tw = pd.read_csv('tw_BertVector_MSFT.csv')\n",
    "featureDF_tw = pd.read_csv('tw_feature_woText_MSFT.csv')\n",
    "featureFI = pd.read_csv('FIwithTargetwithFTT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:25:17.228191Z",
     "start_time": "2019-05-20T22:25:17.222209Z"
    }
   },
   "outputs": [],
   "source": [
    "tw_date = featureDF_tw.date.values.tolist()\n",
    "bert_featureDF_tw['date'] = tw_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:25:17.469987Z",
     "start_time": "2019-05-20T22:25:17.439072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015/05/04\n",
      "endDate 2019/04/04\n",
      "shape (1426, 770)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "targetAndDateDF = featureFI[['date','Target']]\n",
    "result_tw =pd.merge( targetAndDateDF,bert_featureDF_tw, how='right', on='date')\n",
    "getStartEndDateShape(result_tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:25:17.835928Z",
     "start_time": "2019-05-20T22:25:17.785099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015/05/04\n",
      "endDate 2019/04/04\n",
      "shape (1426, 770)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "result_tw =pd.merge( targetAndDateDF,bert_featureDF_tw, how='right', on='date')\n",
    "getStartEndDateShape(result_tw)\n",
    "\n",
    "result_tw.sort_values(by='date',inplace=True) \n",
    "cols = ['Target']\n",
    "result_tw[cols] = result_tw[cols].ffill()\n",
    "result_tw.fillna(0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:25:18.574545Z",
     "start_time": "2019-05-20T22:25:18.554563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015/05/04\n",
      "endDate 2019/04/04\n",
      "shape (1426, 770)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "getStartEndDateShape(result_tw)\n",
    "_ens_Twitter_BERT =result_tw = createComparableDataset(result_tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:25:19.690427Z",
     "start_time": "2019-05-20T22:25:19.399220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (1419, 770)\n",
      "---------------------------\n",
      "train (1332, 768) test (87, 768) train_y 1332 test_y 87\n"
     ]
    }
   ],
   "source": [
    "getStartEndDateShape(result_tw)\n",
    "train, train_y,test, test_y = prepareDataSet(result_tw)\n",
    "print('train',train.shape,'test',test.shape,'train_y',len(train_y),'test_y',len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:25:22.910370Z",
     "start_time": "2019-05-20T22:25:22.907379Z"
    }
   },
   "outputs": [],
   "source": [
    "featureLength = train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:27:27.194947Z",
     "start_time": "2019-05-20T22:26:53.186377Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 100)               76900     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 87,101\n",
      "Trainable params: 87,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1198 samples, validate on 134 samples\n",
      "Epoch 1/600\n",
      "1198/1198 [==============================] - 1s 844us/step - loss: 3.2813 - acc: 0.5217 - val_loss: 2.9778 - val_acc: 0.4552\n",
      "Epoch 2/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 2.7683 - acc: 0.5326 - val_loss: 2.5250 - val_acc: 0.4851\n",
      "Epoch 3/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 2.3528 - acc: 0.5534 - val_loss: 2.1451 - val_acc: 0.4925\n",
      "Epoch 4/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 1.9946 - acc: 0.5659 - val_loss: 1.8412 - val_acc: 0.4030\n",
      "Epoch 5/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 1.7248 - acc: 0.5518 - val_loss: 1.6070 - val_acc: 0.5000\n",
      "Epoch 6/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 1.5091 - acc: 0.5518 - val_loss: 1.4270 - val_acc: 0.4627\n",
      "Epoch 7/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 1.3430 - acc: 0.5551 - val_loss: 1.2914 - val_acc: 0.4925\n",
      "Epoch 8/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 1.2167 - acc: 0.5534 - val_loss: 1.1773 - val_acc: 0.4627\n",
      "Epoch 9/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 1.1232 - acc: 0.5518 - val_loss: 1.0869 - val_acc: 0.4851\n",
      "Epoch 10/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 1.0383 - acc: 0.5593 - val_loss: 1.0211 - val_acc: 0.4701\n",
      "Epoch 11/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.9767 - acc: 0.5676 - val_loss: 0.9715 - val_acc: 0.4328\n",
      "Epoch 12/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.9240 - acc: 0.5609 - val_loss: 0.9355 - val_acc: 0.4701\n",
      "Epoch 13/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.8875 - acc: 0.5593 - val_loss: 0.9079 - val_acc: 0.4552\n",
      "Epoch 14/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.8517 - acc: 0.5743 - val_loss: 0.8708 - val_acc: 0.4478\n",
      "Epoch 15/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.8291 - acc: 0.5960 - val_loss: 0.8552 - val_acc: 0.4403\n",
      "Epoch 16/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.8081 - acc: 0.5718 - val_loss: 0.8457 - val_acc: 0.4403\n",
      "Epoch 17/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.7878 - acc: 0.5868 - val_loss: 0.8196 - val_acc: 0.4627\n",
      "Epoch 18/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.7675 - acc: 0.5960 - val_loss: 0.8173 - val_acc: 0.4478\n",
      "Epoch 19/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.7554 - acc: 0.6018 - val_loss: 0.8104 - val_acc: 0.4403\n",
      "Epoch 20/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.7430 - acc: 0.6110 - val_loss: 0.7962 - val_acc: 0.5000\n",
      "Epoch 21/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.7484 - acc: 0.5768 - val_loss: 0.7899 - val_acc: 0.4701\n",
      "Epoch 22/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.7372 - acc: 0.5943 - val_loss: 0.7794 - val_acc: 0.4701\n",
      "Epoch 23/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.7248 - acc: 0.6119 - val_loss: 0.8007 - val_acc: 0.4701\n",
      "Epoch 24/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.7170 - acc: 0.6043 - val_loss: 0.7988 - val_acc: 0.4552\n",
      "Epoch 25/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.7126 - acc: 0.6227 - val_loss: 0.7814 - val_acc: 0.4627\n",
      "Epoch 26/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.7013 - acc: 0.6244 - val_loss: 0.7745 - val_acc: 0.5224\n",
      "Epoch 27/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.7029 - acc: 0.6269 - val_loss: 0.7886 - val_acc: 0.4179\n",
      "Epoch 28/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.6917 - acc: 0.6419 - val_loss: 0.7794 - val_acc: 0.4776\n",
      "Epoch 29/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.6884 - acc: 0.6519 - val_loss: 0.7797 - val_acc: 0.5000\n",
      "Epoch 30/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.6869 - acc: 0.6519 - val_loss: 0.8135 - val_acc: 0.4627\n",
      "Epoch 31/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.6882 - acc: 0.6411 - val_loss: 0.8060 - val_acc: 0.4328\n",
      "Epoch 32/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.6933 - acc: 0.6252 - val_loss: 0.7756 - val_acc: 0.4552\n",
      "Epoch 33/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.6968 - acc: 0.6144 - val_loss: 0.7851 - val_acc: 0.5075\n",
      "Epoch 34/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.6839 - acc: 0.6511 - val_loss: 0.7813 - val_acc: 0.4776\n",
      "Epoch 35/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.6874 - acc: 0.6294 - val_loss: 0.8136 - val_acc: 0.4552\n",
      "Epoch 36/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.6779 - acc: 0.6594 - val_loss: 0.7932 - val_acc: 0.4552\n",
      "Epoch 37/600\n",
      "1198/1198 [==============================] - 0s 52us/step - loss: 0.7002 - acc: 0.6185 - val_loss: 0.7899 - val_acc: 0.4925\n",
      "Epoch 38/600\n",
      "1198/1198 [==============================] - 0s 49us/step - loss: 0.6639 - acc: 0.6703 - val_loss: 0.8059 - val_acc: 0.4552\n",
      "Epoch 39/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.6745 - acc: 0.6377 - val_loss: 0.7964 - val_acc: 0.4851\n",
      "Epoch 40/600\n",
      "1198/1198 [==============================] - 0s 48us/step - loss: 0.6573 - acc: 0.7095 - val_loss: 0.8161 - val_acc: 0.4328\n",
      "Epoch 41/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.6654 - acc: 0.6578 - val_loss: 0.8129 - val_acc: 0.4925\n",
      "Epoch 42/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.6632 - acc: 0.6686 - val_loss: 0.7997 - val_acc: 0.5149\n",
      "Epoch 43/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.6627 - acc: 0.6678 - val_loss: 0.8318 - val_acc: 0.4478\n",
      "Epoch 44/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.6488 - acc: 0.7003 - val_loss: 0.8334 - val_acc: 0.4851\n",
      "Epoch 45/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.6542 - acc: 0.6745 - val_loss: 0.8150 - val_acc: 0.4776\n",
      "Epoch 46/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.6718 - acc: 0.6486 - val_loss: 0.7891 - val_acc: 0.4851\n",
      "Epoch 47/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.6511 - acc: 0.6987 - val_loss: 0.8104 - val_acc: 0.5075\n",
      "Epoch 48/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.6357 - acc: 0.6953 - val_loss: 0.8456 - val_acc: 0.4701\n",
      "Epoch 49/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.6291 - acc: 0.7020 - val_loss: 0.8464 - val_acc: 0.4776\n",
      "Epoch 50/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.6468 - acc: 0.6853 - val_loss: 0.8206 - val_acc: 0.4627\n",
      "Epoch 51/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.6686 - acc: 0.6661 - val_loss: 0.8203 - val_acc: 0.5075\n",
      "Epoch 52/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.6668 - acc: 0.6603 - val_loss: 0.8513 - val_acc: 0.5075\n",
      "Epoch 53/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.6366 - acc: 0.7179 - val_loss: 0.8711 - val_acc: 0.5000\n",
      "Epoch 54/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.6440 - acc: 0.7095 - val_loss: 0.8702 - val_acc: 0.4776\n",
      "Epoch 55/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.6409 - acc: 0.7162 - val_loss: 0.8391 - val_acc: 0.5373\n",
      "Epoch 56/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.6360 - acc: 0.7120 - val_loss: 0.8619 - val_acc: 0.4925\n",
      "Epoch 57/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.6317 - acc: 0.7154 - val_loss: 0.8660 - val_acc: 0.5000\n",
      "Epoch 58/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.6239 - acc: 0.7229 - val_loss: 0.8915 - val_acc: 0.5000\n",
      "Epoch 59/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.6595 - acc: 0.6778 - val_loss: 0.8492 - val_acc: 0.5075\n",
      "Epoch 60/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.6479 - acc: 0.6962 - val_loss: 0.8819 - val_acc: 0.4478\n",
      "Epoch 61/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.6510 - acc: 0.6962 - val_loss: 0.8785 - val_acc: 0.4776\n",
      "Epoch 62/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.6327 - acc: 0.7070 - val_loss: 0.8733 - val_acc: 0.4925\n",
      "Epoch 63/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.6205 - acc: 0.7279 - val_loss: 0.8578 - val_acc: 0.5149\n",
      "Epoch 64/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.6226 - acc: 0.7471 - val_loss: 0.9017 - val_acc: 0.4776\n",
      "Epoch 65/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.6115 - acc: 0.7287 - val_loss: 0.8877 - val_acc: 0.5299\n",
      "Epoch 66/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.6031 - acc: 0.7471 - val_loss: 0.8832 - val_acc: 0.4851\n",
      "Epoch 67/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.5919 - acc: 0.7646 - val_loss: 0.9198 - val_acc: 0.4851\n",
      "Epoch 68/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.6317 - acc: 0.7087 - val_loss: 0.8745 - val_acc: 0.5000\n",
      "Epoch 69/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.6190 - acc: 0.7346 - val_loss: 0.8931 - val_acc: 0.5000\n",
      "Epoch 70/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.6203 - acc: 0.7254 - val_loss: 0.9321 - val_acc: 0.4851\n",
      "Epoch 71/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.6061 - acc: 0.7321 - val_loss: 0.8909 - val_acc: 0.4925\n",
      "Epoch 72/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.6056 - acc: 0.7487 - val_loss: 0.9220 - val_acc: 0.4851\n",
      "Epoch 73/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.6197 - acc: 0.7279 - val_loss: 0.9544 - val_acc: 0.5075\n",
      "Epoch 74/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.5907 - acc: 0.7579 - val_loss: 0.9039 - val_acc: 0.5149\n",
      "Epoch 75/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.5722 - acc: 0.7746 - val_loss: 0.9852 - val_acc: 0.5373\n",
      "Epoch 76/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.5916 - acc: 0.7730 - val_loss: 0.9231 - val_acc: 0.5000\n",
      "Epoch 77/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.5670 - acc: 0.7922 - val_loss: 0.9346 - val_acc: 0.4925\n",
      "Epoch 78/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.5698 - acc: 0.7830 - val_loss: 1.0438 - val_acc: 0.4851\n",
      "Epoch 79/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.6197 - acc: 0.7287 - val_loss: 0.9493 - val_acc: 0.5000\n",
      "Epoch 80/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.5978 - acc: 0.7496 - val_loss: 0.9641 - val_acc: 0.4627\n",
      "Epoch 81/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.5530 - acc: 0.7896 - val_loss: 0.9617 - val_acc: 0.5224\n",
      "Epoch 82/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.5533 - acc: 0.7938 - val_loss: 0.9818 - val_acc: 0.5224\n",
      "Epoch 83/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.5648 - acc: 0.7888 - val_loss: 0.9739 - val_acc: 0.5299\n",
      "Epoch 84/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.5443 - acc: 0.8072 - val_loss: 1.0204 - val_acc: 0.5149\n",
      "Epoch 85/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.5570 - acc: 0.7922 - val_loss: 0.9899 - val_acc: 0.5299\n",
      "Epoch 86/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.5519 - acc: 0.8063 - val_loss: 0.9904 - val_acc: 0.5149\n",
      "Epoch 87/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.5432 - acc: 0.8038 - val_loss: 1.0094 - val_acc: 0.4851\n",
      "Epoch 88/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.5412 - acc: 0.8122 - val_loss: 1.1020 - val_acc: 0.5522\n",
      "Epoch 89/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.5582 - acc: 0.8013 - val_loss: 1.0278 - val_acc: 0.4925\n",
      "Epoch 90/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.5751 - acc: 0.7705 - val_loss: 1.0590 - val_acc: 0.5299\n",
      "Epoch 91/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.5489 - acc: 0.8088 - val_loss: 1.0729 - val_acc: 0.5075\n",
      "Epoch 92/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.5323 - acc: 0.8180 - val_loss: 1.0359 - val_acc: 0.5000\n",
      "Epoch 93/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.5609 - acc: 0.7947 - val_loss: 1.0845 - val_acc: 0.5075\n",
      "Epoch 94/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.5531 - acc: 0.8097 - val_loss: 1.1470 - val_acc: 0.5224\n",
      "Epoch 95/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.5615 - acc: 0.7963 - val_loss: 1.0330 - val_acc: 0.5597\n",
      "Epoch 96/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.5380 - acc: 0.8047 - val_loss: 1.0801 - val_acc: 0.5373\n",
      "Epoch 97/600\n",
      "1198/1198 [==============================] - 0s 49us/step - loss: 0.5241 - acc: 0.8255 - val_loss: 1.0680 - val_acc: 0.5522\n",
      "Epoch 98/600\n",
      "1198/1198 [==============================] - 0s 52us/step - loss: 0.5286 - acc: 0.8122 - val_loss: 1.0503 - val_acc: 0.5448\n",
      "Epoch 99/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.5082 - acc: 0.8564 - val_loss: 1.1564 - val_acc: 0.5373\n",
      "Epoch 100/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.5314 - acc: 0.8122 - val_loss: 1.0592 - val_acc: 0.5075\n",
      "Epoch 101/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.5015 - acc: 0.8564 - val_loss: 1.1924 - val_acc: 0.5000\n",
      "Epoch 102/600\n",
      "1198/1198 [==============================] - 0s 39us/step - loss: 0.5136 - acc: 0.8314 - val_loss: 1.1203 - val_acc: 0.5000\n",
      "Epoch 103/600\n",
      "1198/1198 [==============================] - 0s 40us/step - loss: 0.5381 - acc: 0.8063 - val_loss: 1.0907 - val_acc: 0.5448\n",
      "Epoch 104/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.5307 - acc: 0.8364 - val_loss: 1.1316 - val_acc: 0.5075\n",
      "Epoch 105/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.5231 - acc: 0.8222 - val_loss: 1.1800 - val_acc: 0.5224\n",
      "Epoch 106/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.5157 - acc: 0.8422 - val_loss: 1.1758 - val_acc: 0.5299\n",
      "Epoch 107/600\n",
      "1198/1198 [==============================] - 0s 40us/step - loss: 0.5155 - acc: 0.8372 - val_loss: 1.1598 - val_acc: 0.4925\n",
      "Epoch 108/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4769 - acc: 0.8598 - val_loss: 1.1200 - val_acc: 0.5373\n",
      "Epoch 109/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.4894 - acc: 0.8573 - val_loss: 1.1759 - val_acc: 0.5299\n",
      "Epoch 110/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4886 - acc: 0.8623 - val_loss: 1.2003 - val_acc: 0.4851\n",
      "Epoch 111/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4754 - acc: 0.8765 - val_loss: 1.1541 - val_acc: 0.5299\n",
      "Epoch 112/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4617 - acc: 0.8748 - val_loss: 1.1431 - val_acc: 0.4925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4801 - acc: 0.8773 - val_loss: 1.1581 - val_acc: 0.5597\n",
      "Epoch 114/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4794 - acc: 0.8740 - val_loss: 1.1270 - val_acc: 0.5522\n",
      "Epoch 115/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4756 - acc: 0.8765 - val_loss: 1.1766 - val_acc: 0.5224\n",
      "Epoch 116/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.4684 - acc: 0.8765 - val_loss: 1.1460 - val_acc: 0.5075\n",
      "Epoch 117/600\n",
      "1198/1198 [==============================] - 0s 49us/step - loss: 0.4496 - acc: 0.8957 - val_loss: 1.1943 - val_acc: 0.5224\n",
      "Epoch 118/600\n",
      "1198/1198 [==============================] - 0s 50us/step - loss: 0.4594 - acc: 0.8890 - val_loss: 1.2474 - val_acc: 0.5373\n",
      "Epoch 119/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.4564 - acc: 0.8806 - val_loss: 1.2495 - val_acc: 0.5224\n",
      "Epoch 120/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.4490 - acc: 0.8948 - val_loss: 1.2649 - val_acc: 0.5448\n",
      "Epoch 121/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.4682 - acc: 0.8715 - val_loss: 1.2206 - val_acc: 0.5224\n",
      "Epoch 122/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4551 - acc: 0.8831 - val_loss: 1.2250 - val_acc: 0.5075\n",
      "Epoch 123/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4512 - acc: 0.8965 - val_loss: 1.2400 - val_acc: 0.5224\n",
      "Epoch 124/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.4371 - acc: 0.8948 - val_loss: 1.2516 - val_acc: 0.5299\n",
      "Epoch 125/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4406 - acc: 0.8982 - val_loss: 1.2117 - val_acc: 0.5299\n",
      "Epoch 126/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4461 - acc: 0.8840 - val_loss: 1.2385 - val_acc: 0.5373\n",
      "Epoch 127/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4622 - acc: 0.8740 - val_loss: 1.2153 - val_acc: 0.5299\n",
      "Epoch 128/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.4415 - acc: 0.9098 - val_loss: 1.2790 - val_acc: 0.5224\n",
      "Epoch 129/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.4334 - acc: 0.9065 - val_loss: 1.2081 - val_acc: 0.5597\n",
      "Epoch 130/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4295 - acc: 0.9057 - val_loss: 1.3044 - val_acc: 0.5522\n",
      "Epoch 131/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4177 - acc: 0.9115 - val_loss: 1.2619 - val_acc: 0.5149\n",
      "Epoch 132/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4262 - acc: 0.8982 - val_loss: 1.2838 - val_acc: 0.5149\n",
      "Epoch 133/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4347 - acc: 0.9040 - val_loss: 1.2732 - val_acc: 0.5075\n",
      "Epoch 134/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4329 - acc: 0.8898 - val_loss: 1.2698 - val_acc: 0.5149\n",
      "Epoch 135/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4341 - acc: 0.8940 - val_loss: 1.3746 - val_acc: 0.5149\n",
      "Epoch 136/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.4401 - acc: 0.8982 - val_loss: 1.2600 - val_acc: 0.5149\n",
      "Epoch 137/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.4488 - acc: 0.8881 - val_loss: 1.2775 - val_acc: 0.5224\n",
      "Epoch 138/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4606 - acc: 0.8798 - val_loss: 1.3330 - val_acc: 0.5000\n",
      "Epoch 139/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.4633 - acc: 0.8623 - val_loss: 1.2705 - val_acc: 0.5000\n",
      "Epoch 140/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4560 - acc: 0.8856 - val_loss: 1.3623 - val_acc: 0.4851\n",
      "Epoch 141/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.4241 - acc: 0.9023 - val_loss: 1.2971 - val_acc: 0.4925\n",
      "Epoch 142/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4394 - acc: 0.8898 - val_loss: 1.3188 - val_acc: 0.5000\n",
      "Epoch 143/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.4616 - acc: 0.8756 - val_loss: 1.2348 - val_acc: 0.5299\n",
      "Epoch 144/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4905 - acc: 0.8523 - val_loss: 1.2729 - val_acc: 0.5075\n",
      "Epoch 145/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4614 - acc: 0.8731 - val_loss: 1.2949 - val_acc: 0.4925\n",
      "Epoch 146/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4320 - acc: 0.8998 - val_loss: 1.3336 - val_acc: 0.5149\n",
      "Epoch 147/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4264 - acc: 0.8965 - val_loss: 1.2722 - val_acc: 0.5299\n",
      "Epoch 148/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.4259 - acc: 0.9023 - val_loss: 1.3362 - val_acc: 0.5373\n",
      "Epoch 149/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.4241 - acc: 0.9057 - val_loss: 1.4043 - val_acc: 0.5224\n",
      "Epoch 150/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.4070 - acc: 0.9098 - val_loss: 1.2890 - val_acc: 0.4925\n",
      "Epoch 151/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.3901 - acc: 0.9232 - val_loss: 1.4831 - val_acc: 0.5000\n",
      "Epoch 152/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.4536 - acc: 0.8731 - val_loss: 1.2757 - val_acc: 0.4925\n",
      "Epoch 153/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4214 - acc: 0.8998 - val_loss: 1.3402 - val_acc: 0.4776\n",
      "Epoch 154/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4083 - acc: 0.9107 - val_loss: 1.3353 - val_acc: 0.5448\n",
      "Epoch 155/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.4035 - acc: 0.9124 - val_loss: 1.3312 - val_acc: 0.5224\n",
      "Epoch 156/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.4094 - acc: 0.9190 - val_loss: 1.3716 - val_acc: 0.5149\n",
      "Epoch 157/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.4064 - acc: 0.9182 - val_loss: 1.3662 - val_acc: 0.5299\n",
      "Epoch 158/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.4031 - acc: 0.9207 - val_loss: 1.3680 - val_acc: 0.5224\n",
      "Epoch 159/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.4056 - acc: 0.9023 - val_loss: 1.3264 - val_acc: 0.5448\n",
      "Epoch 160/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.4242 - acc: 0.9040 - val_loss: 1.4146 - val_acc: 0.5224\n",
      "Epoch 161/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.4016 - acc: 0.9290 - val_loss: 1.4028 - val_acc: 0.5000\n",
      "Epoch 162/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3968 - acc: 0.9207 - val_loss: 1.3596 - val_acc: 0.5149\n",
      "Epoch 163/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3841 - acc: 0.9207 - val_loss: 1.4018 - val_acc: 0.5149\n",
      "Epoch 164/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3880 - acc: 0.9240 - val_loss: 1.4148 - val_acc: 0.4925\n",
      "Epoch 165/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3984 - acc: 0.9165 - val_loss: 1.3206 - val_acc: 0.5373\n",
      "Epoch 166/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3976 - acc: 0.9265 - val_loss: 1.3953 - val_acc: 0.5000\n",
      "Epoch 167/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3901 - acc: 0.9257 - val_loss: 1.4729 - val_acc: 0.4925\n",
      "Epoch 168/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3799 - acc: 0.9299 - val_loss: 1.3768 - val_acc: 0.5075\n",
      "Epoch 169/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.3711 - acc: 0.9257 - val_loss: 1.4700 - val_acc: 0.4925\n",
      "Epoch 170/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3833 - acc: 0.9240 - val_loss: 1.3856 - val_acc: 0.5075\n",
      "Epoch 171/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3758 - acc: 0.9332 - val_loss: 1.3900 - val_acc: 0.5448\n",
      "Epoch 172/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3857 - acc: 0.9132 - val_loss: 1.3684 - val_acc: 0.5373\n",
      "Epoch 173/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.3798 - acc: 0.9232 - val_loss: 1.4403 - val_acc: 0.5000\n",
      "Epoch 174/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3772 - acc: 0.9257 - val_loss: 1.4303 - val_acc: 0.5224\n",
      "Epoch 175/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3682 - acc: 0.9282 - val_loss: 1.4732 - val_acc: 0.4776\n",
      "Epoch 176/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3752 - acc: 0.9332 - val_loss: 1.4356 - val_acc: 0.5000\n",
      "Epoch 177/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3647 - acc: 0.9316 - val_loss: 1.4834 - val_acc: 0.5075\n",
      "Epoch 178/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3746 - acc: 0.9299 - val_loss: 1.4768 - val_acc: 0.5149\n",
      "Epoch 179/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3660 - acc: 0.9332 - val_loss: 1.4481 - val_acc: 0.4701\n",
      "Epoch 180/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.3747 - acc: 0.9307 - val_loss: 1.4627 - val_acc: 0.5000\n",
      "Epoch 181/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3691 - acc: 0.9199 - val_loss: 1.4088 - val_acc: 0.5224\n",
      "Epoch 182/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3636 - acc: 0.9357 - val_loss: 1.4664 - val_acc: 0.5075\n",
      "Epoch 183/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3581 - acc: 0.9391 - val_loss: 1.5073 - val_acc: 0.5149\n",
      "Epoch 184/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3899 - acc: 0.9065 - val_loss: 1.4821 - val_acc: 0.5000\n",
      "Epoch 185/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3785 - acc: 0.9316 - val_loss: 1.4077 - val_acc: 0.4851\n",
      "Epoch 186/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3747 - acc: 0.9274 - val_loss: 1.5137 - val_acc: 0.4851\n",
      "Epoch 187/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3769 - acc: 0.9174 - val_loss: 1.3709 - val_acc: 0.4701\n",
      "Epoch 188/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3869 - acc: 0.9165 - val_loss: 1.4215 - val_acc: 0.4627\n",
      "Epoch 189/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3899 - acc: 0.9224 - val_loss: 1.4545 - val_acc: 0.5448\n",
      "Epoch 190/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3871 - acc: 0.9207 - val_loss: 1.4923 - val_acc: 0.4925\n",
      "Epoch 191/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3547 - acc: 0.9316 - val_loss: 1.4878 - val_acc: 0.5000\n",
      "Epoch 192/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3541 - acc: 0.9432 - val_loss: 1.4427 - val_acc: 0.5149\n",
      "Epoch 193/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3515 - acc: 0.9290 - val_loss: 1.5077 - val_acc: 0.4925\n",
      "Epoch 194/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3665 - acc: 0.9332 - val_loss: 1.5103 - val_acc: 0.4851\n",
      "Epoch 195/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3872 - acc: 0.9190 - val_loss: 1.4821 - val_acc: 0.5149\n",
      "Epoch 196/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3863 - acc: 0.9065 - val_loss: 1.4737 - val_acc: 0.4701\n",
      "Epoch 197/600\n",
      "1198/1198 [==============================] - 0s 49us/step - loss: 0.3721 - acc: 0.9341 - val_loss: 1.4510 - val_acc: 0.4925\n",
      "Epoch 198/600\n",
      "1198/1198 [==============================] - 0s 52us/step - loss: 0.3765 - acc: 0.9316 - val_loss: 1.4760 - val_acc: 0.5149\n",
      "Epoch 199/600\n",
      "1198/1198 [==============================] - 0s 58us/step - loss: 0.3577 - acc: 0.9349 - val_loss: 1.4869 - val_acc: 0.5000\n",
      "Epoch 200/600\n",
      "1198/1198 [==============================] - 0s 49us/step - loss: 0.3770 - acc: 0.9232 - val_loss: 1.4679 - val_acc: 0.4925\n",
      "Epoch 201/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3480 - acc: 0.9316 - val_loss: 1.4749 - val_acc: 0.5224\n",
      "Epoch 202/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.3610 - acc: 0.9299 - val_loss: 1.4564 - val_acc: 0.5149\n",
      "Epoch 203/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3645 - acc: 0.9290 - val_loss: 1.5349 - val_acc: 0.5149\n",
      "Epoch 204/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3495 - acc: 0.9282 - val_loss: 1.5052 - val_acc: 0.4552\n",
      "Epoch 205/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3708 - acc: 0.9274 - val_loss: 1.5543 - val_acc: 0.5075\n",
      "Epoch 206/600\n",
      "1198/1198 [==============================] - 0s 48us/step - loss: 0.3577 - acc: 0.9349 - val_loss: 1.4445 - val_acc: 0.5000\n",
      "Epoch 207/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3450 - acc: 0.9366 - val_loss: 1.4275 - val_acc: 0.4925\n",
      "Epoch 208/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.3486 - acc: 0.9382 - val_loss: 1.5013 - val_acc: 0.4776\n",
      "Epoch 209/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.3411 - acc: 0.9416 - val_loss: 1.4959 - val_acc: 0.5224\n",
      "Epoch 210/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3403 - acc: 0.9416 - val_loss: 1.4450 - val_acc: 0.4701\n",
      "Epoch 211/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3381 - acc: 0.9508 - val_loss: 1.4677 - val_acc: 0.5075\n",
      "Epoch 212/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.3335 - acc: 0.9449 - val_loss: 1.4917 - val_acc: 0.4701\n",
      "Epoch 213/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3507 - acc: 0.9449 - val_loss: 1.5652 - val_acc: 0.5000\n",
      "Epoch 214/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3492 - acc: 0.9407 - val_loss: 1.4601 - val_acc: 0.5075\n",
      "Epoch 215/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3367 - acc: 0.9407 - val_loss: 1.4872 - val_acc: 0.5149\n",
      "Epoch 216/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3344 - acc: 0.9432 - val_loss: 1.5102 - val_acc: 0.5075\n",
      "Epoch 217/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3334 - acc: 0.9491 - val_loss: 1.5071 - val_acc: 0.5075\n",
      "Epoch 218/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3504 - acc: 0.9282 - val_loss: 1.4586 - val_acc: 0.4776\n",
      "Epoch 219/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.3861 - acc: 0.9149 - val_loss: 1.4332 - val_acc: 0.4925\n",
      "Epoch 220/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3736 - acc: 0.9199 - val_loss: 1.4775 - val_acc: 0.5000\n",
      "Epoch 221/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3923 - acc: 0.9115 - val_loss: 1.4522 - val_acc: 0.4851\n",
      "Epoch 222/600\n",
      "1198/1198 [==============================] - 0s 49us/step - loss: 0.4125 - acc: 0.8940 - val_loss: 1.5238 - val_acc: 0.4851\n",
      "Epoch 223/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.3831 - acc: 0.9073 - val_loss: 1.4741 - val_acc: 0.4627\n",
      "Epoch 224/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3788 - acc: 0.9174 - val_loss: 1.5225 - val_acc: 0.4478\n",
      "Epoch 225/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3748 - acc: 0.9224 - val_loss: 1.4742 - val_acc: 0.4701\n",
      "Epoch 226/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3736 - acc: 0.9132 - val_loss: 1.4407 - val_acc: 0.4478\n",
      "Epoch 227/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3696 - acc: 0.9232 - val_loss: 1.4122 - val_acc: 0.4925\n",
      "Epoch 228/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3527 - acc: 0.9349 - val_loss: 1.5247 - val_acc: 0.5000\n",
      "Epoch 229/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3696 - acc: 0.9290 - val_loss: 1.5042 - val_acc: 0.5000\n",
      "Epoch 230/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3769 - acc: 0.9282 - val_loss: 1.4301 - val_acc: 0.4925\n",
      "Epoch 231/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3776 - acc: 0.9149 - val_loss: 1.4180 - val_acc: 0.4851\n",
      "Epoch 232/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3543 - acc: 0.9324 - val_loss: 1.5184 - val_acc: 0.5075\n",
      "Epoch 233/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3649 - acc: 0.9316 - val_loss: 1.5463 - val_acc: 0.4776\n",
      "Epoch 234/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3727 - acc: 0.9299 - val_loss: 1.5253 - val_acc: 0.5075\n",
      "Epoch 235/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3553 - acc: 0.9249 - val_loss: 1.5398 - val_acc: 0.5373\n",
      "Epoch 236/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3599 - acc: 0.9240 - val_loss: 1.4770 - val_acc: 0.5224\n",
      "Epoch 237/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3548 - acc: 0.9257 - val_loss: 1.3499 - val_acc: 0.5149\n",
      "Epoch 238/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3526 - acc: 0.9341 - val_loss: 1.5620 - val_acc: 0.5000\n",
      "Epoch 239/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3551 - acc: 0.9240 - val_loss: 1.5039 - val_acc: 0.4627\n",
      "Epoch 240/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3423 - acc: 0.9274 - val_loss: 1.4716 - val_acc: 0.4776\n",
      "Epoch 241/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3309 - acc: 0.9474 - val_loss: 1.5267 - val_acc: 0.5149\n",
      "Epoch 242/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3198 - acc: 0.9541 - val_loss: 1.5007 - val_acc: 0.4851\n",
      "Epoch 243/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3319 - acc: 0.9449 - val_loss: 1.5162 - val_acc: 0.4776\n",
      "Epoch 244/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3274 - acc: 0.9449 - val_loss: 1.5708 - val_acc: 0.5373\n",
      "Epoch 245/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3393 - acc: 0.9391 - val_loss: 1.4461 - val_acc: 0.4851\n",
      "Epoch 246/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3476 - acc: 0.9316 - val_loss: 1.5971 - val_acc: 0.5224\n",
      "Epoch 247/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3400 - acc: 0.9332 - val_loss: 1.5696 - val_acc: 0.5075\n",
      "Epoch 248/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3569 - acc: 0.9240 - val_loss: 1.4841 - val_acc: 0.5000\n",
      "Epoch 249/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3400 - acc: 0.9399 - val_loss: 1.4988 - val_acc: 0.4925\n",
      "Epoch 250/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3325 - acc: 0.9374 - val_loss: 1.5380 - val_acc: 0.4851\n",
      "Epoch 251/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3345 - acc: 0.9391 - val_loss: 1.5306 - val_acc: 0.5149\n",
      "Epoch 252/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3428 - acc: 0.9349 - val_loss: 1.5157 - val_acc: 0.4925\n",
      "Epoch 253/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3532 - acc: 0.9215 - val_loss: 1.5448 - val_acc: 0.5149\n",
      "Epoch 254/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3626 - acc: 0.9257 - val_loss: 1.5408 - val_acc: 0.5373\n",
      "Epoch 255/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3513 - acc: 0.9232 - val_loss: 1.4550 - val_acc: 0.4851\n",
      "Epoch 256/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3344 - acc: 0.9374 - val_loss: 1.5397 - val_acc: 0.4627\n",
      "Epoch 257/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3271 - acc: 0.9399 - val_loss: 1.4596 - val_acc: 0.5149\n",
      "Epoch 258/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3416 - acc: 0.9282 - val_loss: 1.5169 - val_acc: 0.4701\n",
      "Epoch 259/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3443 - acc: 0.9324 - val_loss: 1.6111 - val_acc: 0.4851\n",
      "Epoch 260/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3637 - acc: 0.9107 - val_loss: 1.4639 - val_acc: 0.4478\n",
      "Epoch 261/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3209 - acc: 0.9424 - val_loss: 1.5451 - val_acc: 0.5075\n",
      "Epoch 262/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3315 - acc: 0.9366 - val_loss: 1.5149 - val_acc: 0.4776\n",
      "Epoch 263/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3360 - acc: 0.9324 - val_loss: 1.5591 - val_acc: 0.5075\n",
      "Epoch 264/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3109 - acc: 0.9508 - val_loss: 1.5045 - val_acc: 0.4851\n",
      "Epoch 265/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.3065 - acc: 0.9524 - val_loss: 1.5278 - val_acc: 0.5224\n",
      "Epoch 266/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3025 - acc: 0.9499 - val_loss: 1.5890 - val_acc: 0.5373\n",
      "Epoch 267/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3190 - acc: 0.9499 - val_loss: 1.5317 - val_acc: 0.4851\n",
      "Epoch 268/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3092 - acc: 0.9524 - val_loss: 1.4934 - val_acc: 0.5224\n",
      "Epoch 269/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3374 - acc: 0.9349 - val_loss: 1.5950 - val_acc: 0.5149\n",
      "Epoch 270/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3650 - acc: 0.9174 - val_loss: 1.5142 - val_acc: 0.5149\n",
      "Epoch 271/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3520 - acc: 0.9299 - val_loss: 1.4752 - val_acc: 0.5000\n",
      "Epoch 272/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3439 - acc: 0.9199 - val_loss: 1.4494 - val_acc: 0.4925\n",
      "Epoch 273/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3188 - acc: 0.9466 - val_loss: 1.6475 - val_acc: 0.5000\n",
      "Epoch 274/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3304 - acc: 0.9374 - val_loss: 1.8092 - val_acc: 0.4851\n",
      "Epoch 275/600\n",
      "1198/1198 [==============================] - 0s 48us/step - loss: 0.3683 - acc: 0.9115 - val_loss: 1.5534 - val_acc: 0.5075\n",
      "Epoch 276/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3248 - acc: 0.9482 - val_loss: 1.4785 - val_acc: 0.4925\n",
      "Epoch 277/600\n",
      "1198/1198 [==============================] - 0s 50us/step - loss: 0.3366 - acc: 0.9374 - val_loss: 1.7232 - val_acc: 0.5149\n",
      "Epoch 278/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.3164 - acc: 0.9466 - val_loss: 1.5334 - val_acc: 0.5224\n",
      "Epoch 279/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3222 - acc: 0.9341 - val_loss: 1.5450 - val_acc: 0.4925\n",
      "Epoch 280/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3108 - acc: 0.9541 - val_loss: 1.6033 - val_acc: 0.5000\n",
      "Epoch 281/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3135 - acc: 0.9432 - val_loss: 1.5035 - val_acc: 0.4627\n",
      "Epoch 282/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3275 - acc: 0.9382 - val_loss: 1.6443 - val_acc: 0.5299\n",
      "Epoch 283/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3198 - acc: 0.9474 - val_loss: 1.6445 - val_acc: 0.5373\n",
      "Epoch 284/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3125 - acc: 0.9432 - val_loss: 1.5268 - val_acc: 0.4925\n",
      "Epoch 285/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3248 - acc: 0.9316 - val_loss: 1.5445 - val_acc: 0.4851\n",
      "Epoch 286/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3362 - acc: 0.9332 - val_loss: 1.6051 - val_acc: 0.4776\n",
      "Epoch 287/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3211 - acc: 0.9449 - val_loss: 1.6325 - val_acc: 0.5000\n",
      "Epoch 288/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3170 - acc: 0.9349 - val_loss: 1.5309 - val_acc: 0.5000\n",
      "Epoch 289/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3227 - acc: 0.9424 - val_loss: 1.5530 - val_acc: 0.5000\n",
      "Epoch 290/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3094 - acc: 0.9566 - val_loss: 1.5838 - val_acc: 0.4925\n",
      "Epoch 291/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3174 - acc: 0.9474 - val_loss: 1.6344 - val_acc: 0.4925\n",
      "Epoch 292/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3212 - acc: 0.9424 - val_loss: 1.6294 - val_acc: 0.4701\n",
      "Epoch 293/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3206 - acc: 0.9432 - val_loss: 1.6858 - val_acc: 0.5299\n",
      "Epoch 294/600\n",
      "1198/1198 [==============================] - 0s 40us/step - loss: 0.3110 - acc: 0.9474 - val_loss: 1.4836 - val_acc: 0.5149\n",
      "Epoch 295/600\n",
      "1198/1198 [==============================] - 0s 40us/step - loss: 0.2973 - acc: 0.9533 - val_loss: 1.6513 - val_acc: 0.4478\n",
      "Epoch 296/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3152 - acc: 0.9407 - val_loss: 1.5877 - val_acc: 0.5075\n",
      "Epoch 297/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2999 - acc: 0.9549 - val_loss: 1.5130 - val_acc: 0.5373\n",
      "Epoch 298/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.3142 - acc: 0.9457 - val_loss: 1.5595 - val_acc: 0.4851\n",
      "Epoch 299/600\n",
      "1198/1198 [==============================] - 0s 39us/step - loss: 0.3013 - acc: 0.9508 - val_loss: 1.6123 - val_acc: 0.5224\n",
      "Epoch 300/600\n",
      "1198/1198 [==============================] - 0s 40us/step - loss: 0.3067 - acc: 0.9482 - val_loss: 1.6519 - val_acc: 0.5224\n",
      "Epoch 301/600\n",
      "1198/1198 [==============================] - 0s 40us/step - loss: 0.2945 - acc: 0.9583 - val_loss: 1.5696 - val_acc: 0.5000\n",
      "Epoch 302/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2923 - acc: 0.9583 - val_loss: 1.5695 - val_acc: 0.5000\n",
      "Epoch 303/600\n",
      "1198/1198 [==============================] - 0s 40us/step - loss: 0.2812 - acc: 0.9574 - val_loss: 1.6913 - val_acc: 0.5448\n",
      "Epoch 304/600\n",
      "1198/1198 [==============================] - 0s 40us/step - loss: 0.3070 - acc: 0.9424 - val_loss: 1.5815 - val_acc: 0.4776\n",
      "Epoch 305/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2905 - acc: 0.9524 - val_loss: 1.7078 - val_acc: 0.5075\n",
      "Epoch 306/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3083 - acc: 0.9466 - val_loss: 1.5732 - val_acc: 0.4925\n",
      "Epoch 307/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2989 - acc: 0.9524 - val_loss: 1.6608 - val_acc: 0.5224\n",
      "Epoch 308/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3031 - acc: 0.9549 - val_loss: 1.5629 - val_acc: 0.5000\n",
      "Epoch 309/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3051 - acc: 0.9524 - val_loss: 1.5847 - val_acc: 0.5000\n",
      "Epoch 310/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3061 - acc: 0.9508 - val_loss: 1.6341 - val_acc: 0.5149\n",
      "Epoch 311/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3057 - acc: 0.9441 - val_loss: 1.5244 - val_acc: 0.5149\n",
      "Epoch 312/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3068 - acc: 0.9508 - val_loss: 1.6860 - val_acc: 0.5075\n",
      "Epoch 313/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3222 - acc: 0.9366 - val_loss: 1.6852 - val_acc: 0.4925\n",
      "Epoch 314/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3062 - acc: 0.9482 - val_loss: 1.6990 - val_acc: 0.4851\n",
      "Epoch 315/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3066 - acc: 0.9457 - val_loss: 1.6295 - val_acc: 0.4925\n",
      "Epoch 316/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3044 - acc: 0.9524 - val_loss: 1.6795 - val_acc: 0.5149\n",
      "Epoch 317/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3061 - acc: 0.9482 - val_loss: 1.6491 - val_acc: 0.4925\n",
      "Epoch 318/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3107 - acc: 0.9449 - val_loss: 1.6175 - val_acc: 0.5075\n",
      "Epoch 319/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3083 - acc: 0.9474 - val_loss: 1.7267 - val_acc: 0.5224\n",
      "Epoch 320/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2931 - acc: 0.9541 - val_loss: 1.6743 - val_acc: 0.4552\n",
      "Epoch 321/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3207 - acc: 0.9382 - val_loss: 1.7255 - val_acc: 0.5075\n",
      "Epoch 322/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3297 - acc: 0.9349 - val_loss: 1.6267 - val_acc: 0.5224\n",
      "Epoch 323/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3196 - acc: 0.9382 - val_loss: 1.6237 - val_acc: 0.5000\n",
      "Epoch 324/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3048 - acc: 0.9457 - val_loss: 1.6611 - val_acc: 0.4851\n",
      "Epoch 325/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2964 - acc: 0.9516 - val_loss: 1.6998 - val_acc: 0.4776\n",
      "Epoch 326/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2901 - acc: 0.9591 - val_loss: 1.6707 - val_acc: 0.5373\n",
      "Epoch 327/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3071 - acc: 0.9457 - val_loss: 1.5262 - val_acc: 0.4851\n",
      "Epoch 328/600\n",
      "1198/1198 [==============================] - 0s 40us/step - loss: 0.3135 - acc: 0.9374 - val_loss: 1.5856 - val_acc: 0.5373\n",
      "Epoch 329/600\n",
      "1198/1198 [==============================] - 0s 40us/step - loss: 0.3148 - acc: 0.9424 - val_loss: 1.6230 - val_acc: 0.5224\n",
      "Epoch 330/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.3217 - acc: 0.9416 - val_loss: 1.7058 - val_acc: 0.5000\n",
      "Epoch 331/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.3180 - acc: 0.9307 - val_loss: 1.5958 - val_acc: 0.4925\n",
      "Epoch 332/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.3124 - acc: 0.9341 - val_loss: 1.4967 - val_acc: 0.4701\n",
      "Epoch 333/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3290 - acc: 0.9274 - val_loss: 1.6915 - val_acc: 0.5224\n",
      "Epoch 334/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3326 - acc: 0.9274 - val_loss: 1.6311 - val_acc: 0.4925\n",
      "Epoch 335/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2888 - acc: 0.9508 - val_loss: 1.7042 - val_acc: 0.5373\n",
      "Epoch 336/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2998 - acc: 0.9491 - val_loss: 1.5843 - val_acc: 0.5075\n",
      "Epoch 337/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2884 - acc: 0.9474 - val_loss: 1.5684 - val_acc: 0.5075\n",
      "Epoch 338/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2924 - acc: 0.9549 - val_loss: 1.6401 - val_acc: 0.4925\n",
      "Epoch 339/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3021 - acc: 0.9449 - val_loss: 1.6866 - val_acc: 0.5224\n",
      "Epoch 340/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2975 - acc: 0.9516 - val_loss: 1.6119 - val_acc: 0.5149\n",
      "Epoch 341/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3081 - acc: 0.9474 - val_loss: 1.6169 - val_acc: 0.5373\n",
      "Epoch 342/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2993 - acc: 0.9499 - val_loss: 1.6763 - val_acc: 0.5448\n",
      "Epoch 343/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2843 - acc: 0.9566 - val_loss: 1.6191 - val_acc: 0.5299\n",
      "Epoch 344/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3009 - acc: 0.9533 - val_loss: 1.7420 - val_acc: 0.5373\n",
      "Epoch 345/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2775 - acc: 0.9574 - val_loss: 1.5782 - val_acc: 0.4851\n",
      "Epoch 346/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3051 - acc: 0.9432 - val_loss: 1.5957 - val_acc: 0.4627\n",
      "Epoch 347/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2881 - acc: 0.9516 - val_loss: 1.7320 - val_acc: 0.5448\n",
      "Epoch 348/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2806 - acc: 0.9533 - val_loss: 1.6423 - val_acc: 0.4925\n",
      "Epoch 349/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2849 - acc: 0.9499 - val_loss: 1.6634 - val_acc: 0.5149\n",
      "Epoch 350/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2889 - acc: 0.9474 - val_loss: 1.6413 - val_acc: 0.5075\n",
      "Epoch 351/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2728 - acc: 0.9566 - val_loss: 1.6845 - val_acc: 0.4851\n",
      "Epoch 352/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2801 - acc: 0.9574 - val_loss: 1.7360 - val_acc: 0.5149\n",
      "Epoch 353/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2820 - acc: 0.9491 - val_loss: 1.7356 - val_acc: 0.5299\n",
      "Epoch 354/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2893 - acc: 0.9466 - val_loss: 1.6959 - val_acc: 0.5224\n",
      "Epoch 355/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.2981 - acc: 0.9407 - val_loss: 1.6191 - val_acc: 0.5373\n",
      "Epoch 356/600\n",
      "1198/1198 [==============================] - 0s 49us/step - loss: 0.3001 - acc: 0.9382 - val_loss: 1.6884 - val_acc: 0.5000\n",
      "Epoch 357/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.2861 - acc: 0.9533 - val_loss: 1.6806 - val_acc: 0.5299\n",
      "Epoch 358/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2952 - acc: 0.9482 - val_loss: 1.6434 - val_acc: 0.5299\n",
      "Epoch 359/600\n",
      "1198/1198 [==============================] - 0s 39us/step - loss: 0.2897 - acc: 0.9499 - val_loss: 1.6541 - val_acc: 0.5373\n",
      "Epoch 360/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3052 - acc: 0.9366 - val_loss: 1.4324 - val_acc: 0.4627\n",
      "Epoch 361/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3191 - acc: 0.9382 - val_loss: 1.6450 - val_acc: 0.4851\n",
      "Epoch 362/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3019 - acc: 0.9491 - val_loss: 1.6922 - val_acc: 0.5075\n",
      "Epoch 363/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2887 - acc: 0.9499 - val_loss: 1.6433 - val_acc: 0.4925\n",
      "Epoch 364/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2897 - acc: 0.9474 - val_loss: 1.5959 - val_acc: 0.5000\n",
      "Epoch 365/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2921 - acc: 0.9541 - val_loss: 1.6336 - val_acc: 0.5000\n",
      "Epoch 366/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2971 - acc: 0.9432 - val_loss: 1.6596 - val_acc: 0.5149\n",
      "Epoch 367/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2955 - acc: 0.9432 - val_loss: 1.6505 - val_acc: 0.5149\n",
      "Epoch 368/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2842 - acc: 0.9541 - val_loss: 1.6479 - val_acc: 0.4478\n",
      "Epoch 369/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2989 - acc: 0.9449 - val_loss: 1.8171 - val_acc: 0.5000\n",
      "Epoch 370/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3019 - acc: 0.9441 - val_loss: 1.5762 - val_acc: 0.5597\n",
      "Epoch 371/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3039 - acc: 0.9416 - val_loss: 1.6198 - val_acc: 0.5149\n",
      "Epoch 372/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3008 - acc: 0.9457 - val_loss: 1.6921 - val_acc: 0.5075\n",
      "Epoch 373/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2875 - acc: 0.9491 - val_loss: 1.5886 - val_acc: 0.4627\n",
      "Epoch 374/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2815 - acc: 0.9558 - val_loss: 1.6620 - val_acc: 0.4925\n",
      "Epoch 375/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3078 - acc: 0.9357 - val_loss: 1.6334 - val_acc: 0.5075\n",
      "Epoch 376/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2949 - acc: 0.9516 - val_loss: 1.7642 - val_acc: 0.5522\n",
      "Epoch 377/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2824 - acc: 0.9541 - val_loss: 1.6956 - val_acc: 0.5075\n",
      "Epoch 378/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2819 - acc: 0.9549 - val_loss: 1.7331 - val_acc: 0.5149\n",
      "Epoch 379/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2822 - acc: 0.9524 - val_loss: 1.7314 - val_acc: 0.4776\n",
      "Epoch 380/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2949 - acc: 0.9499 - val_loss: 1.6367 - val_acc: 0.5075\n",
      "Epoch 381/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2693 - acc: 0.9574 - val_loss: 1.7414 - val_acc: 0.5224\n",
      "Epoch 382/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2877 - acc: 0.9508 - val_loss: 1.6981 - val_acc: 0.5299\n",
      "Epoch 383/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2829 - acc: 0.9574 - val_loss: 1.6275 - val_acc: 0.5373\n",
      "Epoch 384/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2742 - acc: 0.9608 - val_loss: 1.7193 - val_acc: 0.5000\n",
      "Epoch 385/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3404 - acc: 0.9249 - val_loss: 1.7797 - val_acc: 0.4925\n",
      "Epoch 386/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.3114 - acc: 0.9324 - val_loss: 1.6309 - val_acc: 0.4627\n",
      "Epoch 387/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3075 - acc: 0.9466 - val_loss: 1.6132 - val_acc: 0.5075\n",
      "Epoch 388/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2917 - acc: 0.9533 - val_loss: 1.6841 - val_acc: 0.4851\n",
      "Epoch 389/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3039 - acc: 0.9424 - val_loss: 1.6797 - val_acc: 0.4851\n",
      "Epoch 390/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3052 - acc: 0.9399 - val_loss: 1.8553 - val_acc: 0.4925\n",
      "Epoch 391/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2876 - acc: 0.9424 - val_loss: 1.7270 - val_acc: 0.5075\n",
      "Epoch 392/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2710 - acc: 0.9549 - val_loss: 1.7717 - val_acc: 0.5299\n",
      "Epoch 393/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2608 - acc: 0.9666 - val_loss: 1.8514 - val_acc: 0.5224\n",
      "Epoch 394/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2925 - acc: 0.9407 - val_loss: 1.6659 - val_acc: 0.5299\n",
      "Epoch 395/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2770 - acc: 0.9541 - val_loss: 1.7142 - val_acc: 0.4851\n",
      "Epoch 396/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2930 - acc: 0.9508 - val_loss: 1.7894 - val_acc: 0.5149\n",
      "Epoch 397/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2900 - acc: 0.9424 - val_loss: 1.7332 - val_acc: 0.5000\n",
      "Epoch 398/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2937 - acc: 0.9482 - val_loss: 1.7129 - val_acc: 0.5075\n",
      "Epoch 399/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2922 - acc: 0.9499 - val_loss: 1.6866 - val_acc: 0.5149\n",
      "Epoch 400/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3070 - acc: 0.9341 - val_loss: 1.6339 - val_acc: 0.5075\n",
      "Epoch 401/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3051 - acc: 0.9366 - val_loss: 1.7226 - val_acc: 0.5149\n",
      "Epoch 402/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2848 - acc: 0.9524 - val_loss: 1.7301 - val_acc: 0.5448\n",
      "Epoch 403/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2680 - acc: 0.9591 - val_loss: 1.7966 - val_acc: 0.5149\n",
      "Epoch 404/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2868 - acc: 0.9499 - val_loss: 1.7447 - val_acc: 0.4851\n",
      "Epoch 405/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2826 - acc: 0.9491 - val_loss: 1.8142 - val_acc: 0.5149\n",
      "Epoch 406/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2856 - acc: 0.9524 - val_loss: 1.6601 - val_acc: 0.5448\n",
      "Epoch 407/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2917 - acc: 0.9457 - val_loss: 1.7385 - val_acc: 0.4851\n",
      "Epoch 408/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2843 - acc: 0.9491 - val_loss: 1.6997 - val_acc: 0.5149\n",
      "Epoch 409/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2771 - acc: 0.9533 - val_loss: 1.7431 - val_acc: 0.5299\n",
      "Epoch 410/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2794 - acc: 0.9533 - val_loss: 1.7170 - val_acc: 0.4776\n",
      "Epoch 411/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.2791 - acc: 0.9449 - val_loss: 1.7114 - val_acc: 0.4925\n",
      "Epoch 412/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2822 - acc: 0.9533 - val_loss: 1.8281 - val_acc: 0.5597\n",
      "Epoch 413/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2796 - acc: 0.9616 - val_loss: 1.7651 - val_acc: 0.5000\n",
      "Epoch 414/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2897 - acc: 0.9508 - val_loss: 1.8416 - val_acc: 0.4925\n",
      "Epoch 415/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2912 - acc: 0.9424 - val_loss: 1.7611 - val_acc: 0.5149\n",
      "Epoch 416/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2731 - acc: 0.9566 - val_loss: 1.7524 - val_acc: 0.5299\n",
      "Epoch 417/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2766 - acc: 0.9508 - val_loss: 1.7655 - val_acc: 0.5075\n",
      "Epoch 418/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2784 - acc: 0.9516 - val_loss: 1.7103 - val_acc: 0.4627\n",
      "Epoch 419/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.2882 - acc: 0.9466 - val_loss: 1.8202 - val_acc: 0.4851\n",
      "Epoch 420/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2796 - acc: 0.9508 - val_loss: 1.7631 - val_acc: 0.4925\n",
      "Epoch 421/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3132 - acc: 0.9282 - val_loss: 1.6499 - val_acc: 0.4925\n",
      "Epoch 422/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.2980 - acc: 0.9449 - val_loss: 1.6517 - val_acc: 0.4925\n",
      "Epoch 423/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2884 - acc: 0.9499 - val_loss: 1.8209 - val_acc: 0.4851\n",
      "Epoch 424/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3287 - acc: 0.9224 - val_loss: 1.8490 - val_acc: 0.5075\n",
      "Epoch 425/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3217 - acc: 0.9265 - val_loss: 1.7091 - val_acc: 0.4776\n",
      "Epoch 426/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3388 - acc: 0.9224 - val_loss: 1.5950 - val_acc: 0.5149\n",
      "Epoch 427/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3161 - acc: 0.9215 - val_loss: 1.5341 - val_acc: 0.5299\n",
      "Epoch 428/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3140 - acc: 0.9332 - val_loss: 1.5996 - val_acc: 0.4925\n",
      "Epoch 429/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3123 - acc: 0.9407 - val_loss: 1.6838 - val_acc: 0.4776\n",
      "Epoch 430/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3064 - acc: 0.9357 - val_loss: 1.7490 - val_acc: 0.5149\n",
      "Epoch 431/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2969 - acc: 0.9441 - val_loss: 1.6396 - val_acc: 0.5373\n",
      "Epoch 432/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2907 - acc: 0.9441 - val_loss: 1.7580 - val_acc: 0.5075\n",
      "Epoch 433/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2923 - acc: 0.9391 - val_loss: 1.6749 - val_acc: 0.5000\n",
      "Epoch 434/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3117 - acc: 0.9382 - val_loss: 1.8024 - val_acc: 0.4776\n",
      "Epoch 435/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2814 - acc: 0.9566 - val_loss: 1.6421 - val_acc: 0.4925\n",
      "Epoch 436/600\n",
      "1198/1198 [==============================] - 0s 50us/step - loss: 0.2993 - acc: 0.9416 - val_loss: 1.7308 - val_acc: 0.4851\n",
      "Epoch 437/600\n",
      "1198/1198 [==============================] - 0s 51us/step - loss: 0.3212 - acc: 0.9224 - val_loss: 1.7185 - val_acc: 0.4851\n",
      "Epoch 438/600\n",
      "1198/1198 [==============================] - 0s 51us/step - loss: 0.3171 - acc: 0.9274 - val_loss: 1.6919 - val_acc: 0.5075\n",
      "Epoch 439/600\n",
      "1198/1198 [==============================] - 0s 48us/step - loss: 0.2833 - acc: 0.9474 - val_loss: 1.6918 - val_acc: 0.5149\n",
      "Epoch 440/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2941 - acc: 0.9533 - val_loss: 1.7737 - val_acc: 0.5000\n",
      "Epoch 441/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2983 - acc: 0.9424 - val_loss: 1.6937 - val_acc: 0.4552\n",
      "Epoch 442/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2925 - acc: 0.9482 - val_loss: 1.6851 - val_acc: 0.5149\n",
      "Epoch 443/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2881 - acc: 0.9449 - val_loss: 1.6672 - val_acc: 0.4627\n",
      "Epoch 444/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.2804 - acc: 0.9466 - val_loss: 1.6887 - val_acc: 0.4627\n",
      "Epoch 445/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2917 - acc: 0.9416 - val_loss: 1.6690 - val_acc: 0.5000\n",
      "Epoch 446/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2758 - acc: 0.9533 - val_loss: 1.7469 - val_acc: 0.5000\n",
      "Epoch 447/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2845 - acc: 0.9516 - val_loss: 1.8907 - val_acc: 0.5149\n",
      "Epoch 448/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2928 - acc: 0.9432 - val_loss: 1.7251 - val_acc: 0.5299\n",
      "Epoch 449/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2906 - acc: 0.9416 - val_loss: 1.6818 - val_acc: 0.5299\n",
      "Epoch 450/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2827 - acc: 0.9474 - val_loss: 1.8055 - val_acc: 0.5224\n",
      "Epoch 451/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2854 - acc: 0.9441 - val_loss: 1.8185 - val_acc: 0.4627\n",
      "Epoch 452/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2863 - acc: 0.9549 - val_loss: 1.8038 - val_acc: 0.4925\n",
      "Epoch 453/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2774 - acc: 0.9566 - val_loss: 1.8073 - val_acc: 0.4478\n",
      "Epoch 454/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2764 - acc: 0.9549 - val_loss: 1.7424 - val_acc: 0.4776\n",
      "Epoch 455/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2571 - acc: 0.9641 - val_loss: 1.7831 - val_acc: 0.5224\n",
      "Epoch 456/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2718 - acc: 0.9549 - val_loss: 1.7119 - val_acc: 0.5000\n",
      "Epoch 457/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2590 - acc: 0.9558 - val_loss: 1.7291 - val_acc: 0.5075\n",
      "Epoch 458/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2616 - acc: 0.9549 - val_loss: 1.8190 - val_acc: 0.5000\n",
      "Epoch 459/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2655 - acc: 0.9516 - val_loss: 1.7530 - val_acc: 0.4925\n",
      "Epoch 460/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2568 - acc: 0.9574 - val_loss: 1.7873 - val_acc: 0.5224\n",
      "Epoch 461/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2638 - acc: 0.9574 - val_loss: 1.7543 - val_acc: 0.5000\n",
      "Epoch 462/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2781 - acc: 0.9466 - val_loss: 1.7418 - val_acc: 0.4552\n",
      "Epoch 463/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2850 - acc: 0.9391 - val_loss: 1.7274 - val_acc: 0.5224\n",
      "Epoch 464/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3019 - acc: 0.9349 - val_loss: 1.8133 - val_acc: 0.5000\n",
      "Epoch 465/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.3020 - acc: 0.9349 - val_loss: 1.7970 - val_acc: 0.5075\n",
      "Epoch 466/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2864 - acc: 0.9416 - val_loss: 1.7908 - val_acc: 0.5149\n",
      "Epoch 467/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2639 - acc: 0.9574 - val_loss: 1.7088 - val_acc: 0.5075\n",
      "Epoch 468/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3159 - acc: 0.9316 - val_loss: 1.7587 - val_acc: 0.5224\n",
      "Epoch 469/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2968 - acc: 0.9316 - val_loss: 1.7567 - val_acc: 0.4925\n",
      "Epoch 470/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2730 - acc: 0.9533 - val_loss: 1.7928 - val_acc: 0.4776\n",
      "Epoch 471/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2678 - acc: 0.9508 - val_loss: 1.7929 - val_acc: 0.5000\n",
      "Epoch 472/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2628 - acc: 0.9583 - val_loss: 1.7514 - val_acc: 0.4925\n",
      "Epoch 473/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2631 - acc: 0.9549 - val_loss: 1.7718 - val_acc: 0.4925\n",
      "Epoch 474/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2665 - acc: 0.9499 - val_loss: 1.9137 - val_acc: 0.5299\n",
      "Epoch 475/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2596 - acc: 0.9533 - val_loss: 1.8456 - val_acc: 0.4851\n",
      "Epoch 476/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2535 - acc: 0.9616 - val_loss: 1.8268 - val_acc: 0.5224\n",
      "Epoch 477/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2593 - acc: 0.9566 - val_loss: 1.7340 - val_acc: 0.4925\n",
      "Epoch 478/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2486 - acc: 0.9574 - val_loss: 1.8308 - val_acc: 0.5000\n",
      "Epoch 479/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2492 - acc: 0.9633 - val_loss: 1.8867 - val_acc: 0.5000\n",
      "Epoch 480/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2544 - acc: 0.9558 - val_loss: 1.8193 - val_acc: 0.4851\n",
      "Epoch 481/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2707 - acc: 0.9541 - val_loss: 1.8197 - val_acc: 0.5149\n",
      "Epoch 482/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2706 - acc: 0.9491 - val_loss: 1.8029 - val_acc: 0.4478\n",
      "Epoch 483/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2675 - acc: 0.9499 - val_loss: 1.7355 - val_acc: 0.5299\n",
      "Epoch 484/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2781 - acc: 0.9474 - val_loss: 1.7659 - val_acc: 0.5000\n",
      "Epoch 485/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2691 - acc: 0.9499 - val_loss: 1.7878 - val_acc: 0.5000\n",
      "Epoch 486/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2869 - acc: 0.9399 - val_loss: 1.7390 - val_acc: 0.5075\n",
      "Epoch 487/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2931 - acc: 0.9366 - val_loss: 1.7975 - val_acc: 0.5075\n",
      "Epoch 488/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2657 - acc: 0.9599 - val_loss: 1.8473 - val_acc: 0.5075\n",
      "Epoch 489/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2649 - acc: 0.9482 - val_loss: 1.8152 - val_acc: 0.5075\n",
      "Epoch 490/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2669 - acc: 0.9533 - val_loss: 1.9173 - val_acc: 0.5299\n",
      "Epoch 491/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2673 - acc: 0.9516 - val_loss: 1.6960 - val_acc: 0.5149\n",
      "Epoch 492/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2690 - acc: 0.9533 - val_loss: 1.7684 - val_acc: 0.5448\n",
      "Epoch 493/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2773 - acc: 0.9482 - val_loss: 1.8516 - val_acc: 0.5149\n",
      "Epoch 494/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2892 - acc: 0.9441 - val_loss: 1.7844 - val_acc: 0.4701\n",
      "Epoch 495/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2809 - acc: 0.9466 - val_loss: 1.8006 - val_acc: 0.5224\n",
      "Epoch 496/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2829 - acc: 0.9449 - val_loss: 1.7198 - val_acc: 0.4701\n",
      "Epoch 497/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2803 - acc: 0.9424 - val_loss: 1.8305 - val_acc: 0.5000\n",
      "Epoch 498/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2558 - acc: 0.9591 - val_loss: 1.7400 - val_acc: 0.5224\n",
      "Epoch 499/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2631 - acc: 0.9591 - val_loss: 1.7196 - val_acc: 0.4776\n",
      "Epoch 500/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2820 - acc: 0.9374 - val_loss: 1.7711 - val_acc: 0.5075\n",
      "Epoch 501/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2645 - acc: 0.9499 - val_loss: 1.8660 - val_acc: 0.5522\n",
      "Epoch 502/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2601 - acc: 0.9591 - val_loss: 1.8491 - val_acc: 0.5075\n",
      "Epoch 503/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2643 - acc: 0.9549 - val_loss: 1.8240 - val_acc: 0.4776\n",
      "Epoch 504/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2750 - acc: 0.9499 - val_loss: 1.8678 - val_acc: 0.4851\n",
      "Epoch 505/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2480 - acc: 0.9624 - val_loss: 1.9236 - val_acc: 0.4851\n",
      "Epoch 506/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2603 - acc: 0.9558 - val_loss: 1.8432 - val_acc: 0.4627\n",
      "Epoch 507/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2710 - acc: 0.9457 - val_loss: 1.7374 - val_acc: 0.4851\n",
      "Epoch 508/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2714 - acc: 0.9524 - val_loss: 1.8691 - val_acc: 0.5000\n",
      "Epoch 509/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2657 - acc: 0.9482 - val_loss: 1.8087 - val_acc: 0.4776\n",
      "Epoch 510/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.3035 - acc: 0.9290 - val_loss: 1.6992 - val_acc: 0.4552\n",
      "Epoch 511/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2956 - acc: 0.9341 - val_loss: 1.7224 - val_acc: 0.5299\n",
      "Epoch 512/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2888 - acc: 0.9407 - val_loss: 1.7302 - val_acc: 0.4776\n",
      "Epoch 513/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2970 - acc: 0.9374 - val_loss: 1.6980 - val_acc: 0.5000\n",
      "Epoch 514/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2662 - acc: 0.9549 - val_loss: 1.7545 - val_acc: 0.4925\n",
      "Epoch 515/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2684 - acc: 0.9457 - val_loss: 1.9126 - val_acc: 0.5000\n",
      "Epoch 516/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.2721 - acc: 0.9508 - val_loss: 1.7736 - val_acc: 0.5224\n",
      "Epoch 517/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.2565 - acc: 0.9549 - val_loss: 1.7866 - val_acc: 0.4627\n",
      "Epoch 518/600\n",
      "1198/1198 [==============================] - 0s 47us/step - loss: 0.2695 - acc: 0.9449 - val_loss: 1.9027 - val_acc: 0.5373\n",
      "Epoch 519/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2697 - acc: 0.9508 - val_loss: 1.8133 - val_acc: 0.4776\n",
      "Epoch 520/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.2538 - acc: 0.9583 - val_loss: 1.7929 - val_acc: 0.4627\n",
      "Epoch 521/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2513 - acc: 0.9633 - val_loss: 2.0075 - val_acc: 0.5149\n",
      "Epoch 522/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2498 - acc: 0.9616 - val_loss: 1.7372 - val_acc: 0.5000\n",
      "Epoch 523/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2626 - acc: 0.9482 - val_loss: 1.8446 - val_acc: 0.5000\n",
      "Epoch 524/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2584 - acc: 0.9566 - val_loss: 1.9026 - val_acc: 0.5299\n",
      "Epoch 525/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2808 - acc: 0.9424 - val_loss: 1.7693 - val_acc: 0.4776\n",
      "Epoch 526/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2890 - acc: 0.9332 - val_loss: 1.7111 - val_acc: 0.5373\n",
      "Epoch 527/600\n",
      "1198/1198 [==============================] - 0s 40us/step - loss: 0.3164 - acc: 0.9207 - val_loss: 1.7848 - val_acc: 0.5075\n",
      "Epoch 528/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.3004 - acc: 0.9316 - val_loss: 1.6401 - val_acc: 0.4627\n",
      "Epoch 529/600\n",
      "1198/1198 [==============================] - 0s 39us/step - loss: 0.3061 - acc: 0.9240 - val_loss: 1.7074 - val_acc: 0.5149\n",
      "Epoch 530/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.2935 - acc: 0.9299 - val_loss: 1.7213 - val_acc: 0.5075\n",
      "Epoch 531/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.2932 - acc: 0.9341 - val_loss: 1.7066 - val_acc: 0.4851\n",
      "Epoch 532/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.2558 - acc: 0.9591 - val_loss: 1.7560 - val_acc: 0.4627\n",
      "Epoch 533/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2739 - acc: 0.9466 - val_loss: 1.7539 - val_acc: 0.5075\n",
      "Epoch 534/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2630 - acc: 0.9516 - val_loss: 1.6739 - val_acc: 0.5000\n",
      "Epoch 535/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2957 - acc: 0.9399 - val_loss: 1.7743 - val_acc: 0.4627\n",
      "Epoch 536/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2735 - acc: 0.9432 - val_loss: 1.8021 - val_acc: 0.5373\n",
      "Epoch 537/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2757 - acc: 0.9482 - val_loss: 1.6826 - val_acc: 0.4851\n",
      "Epoch 538/600\n",
      "1198/1198 [==============================] - 0s 51us/step - loss: 0.2681 - acc: 0.9474 - val_loss: 1.7391 - val_acc: 0.4851\n",
      "Epoch 539/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2793 - acc: 0.9449 - val_loss: 1.7108 - val_acc: 0.4776\n",
      "Epoch 540/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2578 - acc: 0.9508 - val_loss: 1.6731 - val_acc: 0.5000\n",
      "Epoch 541/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2699 - acc: 0.9432 - val_loss: 1.7344 - val_acc: 0.4925\n",
      "Epoch 542/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2793 - acc: 0.9474 - val_loss: 1.5977 - val_acc: 0.4851\n",
      "Epoch 543/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2829 - acc: 0.9482 - val_loss: 1.8096 - val_acc: 0.5075\n",
      "Epoch 544/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.3020 - acc: 0.9324 - val_loss: 1.7741 - val_acc: 0.4627\n",
      "Epoch 545/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2843 - acc: 0.9316 - val_loss: 1.6643 - val_acc: 0.4925\n",
      "Epoch 546/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2832 - acc: 0.9466 - val_loss: 1.7399 - val_acc: 0.5000\n",
      "Epoch 547/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2778 - acc: 0.9391 - val_loss: 1.7189 - val_acc: 0.5000\n",
      "Epoch 548/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2749 - acc: 0.9416 - val_loss: 1.6600 - val_acc: 0.4627\n",
      "Epoch 549/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2537 - acc: 0.9549 - val_loss: 1.7956 - val_acc: 0.5149\n",
      "Epoch 550/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2464 - acc: 0.9599 - val_loss: 1.8550 - val_acc: 0.4851\n",
      "Epoch 551/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2756 - acc: 0.9449 - val_loss: 1.7611 - val_acc: 0.4851\n",
      "Epoch 552/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2532 - acc: 0.9566 - val_loss: 1.7413 - val_acc: 0.4478\n",
      "Epoch 553/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2548 - acc: 0.9566 - val_loss: 1.7667 - val_acc: 0.5000\n",
      "Epoch 554/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2731 - acc: 0.9457 - val_loss: 1.9062 - val_acc: 0.4776\n",
      "Epoch 555/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2539 - acc: 0.9482 - val_loss: 1.8042 - val_acc: 0.4701\n",
      "Epoch 556/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2711 - acc: 0.9432 - val_loss: 1.8446 - val_acc: 0.4925\n",
      "Epoch 557/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2511 - acc: 0.9633 - val_loss: 1.8596 - val_acc: 0.4478\n",
      "Epoch 558/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2526 - acc: 0.9566 - val_loss: 1.8359 - val_acc: 0.4851\n",
      "Epoch 559/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2395 - acc: 0.9574 - val_loss: 1.8118 - val_acc: 0.4701\n",
      "Epoch 560/600\n",
      "1198/1198 [==============================] - 0s 41us/step - loss: 0.2415 - acc: 0.9608 - val_loss: 1.9029 - val_acc: 0.4701\n",
      "Epoch 561/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2615 - acc: 0.9558 - val_loss: 1.9457 - val_acc: 0.4701\n",
      "Epoch 562/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2706 - acc: 0.9466 - val_loss: 1.8557 - val_acc: 0.4552\n",
      "Epoch 563/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2991 - acc: 0.9290 - val_loss: 1.8402 - val_acc: 0.4627\n",
      "Epoch 564/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2769 - acc: 0.9482 - val_loss: 1.8295 - val_acc: 0.4701\n",
      "Epoch 565/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2693 - acc: 0.9432 - val_loss: 1.7432 - val_acc: 0.5000\n",
      "Epoch 566/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2783 - acc: 0.9432 - val_loss: 1.8034 - val_acc: 0.5224\n",
      "Epoch 567/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2663 - acc: 0.9407 - val_loss: 1.7744 - val_acc: 0.4627\n",
      "Epoch 568/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2630 - acc: 0.9399 - val_loss: 1.6322 - val_acc: 0.4403\n",
      "Epoch 569/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2659 - acc: 0.9441 - val_loss: 1.6809 - val_acc: 0.5000\n",
      "Epoch 570/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2573 - acc: 0.9549 - val_loss: 1.7399 - val_acc: 0.4925\n",
      "Epoch 571/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2625 - acc: 0.9508 - val_loss: 1.6358 - val_acc: 0.5000\n",
      "Epoch 572/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2722 - acc: 0.9391 - val_loss: 1.6397 - val_acc: 0.5075\n",
      "Epoch 573/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2594 - acc: 0.9516 - val_loss: 1.6589 - val_acc: 0.5075\n",
      "Epoch 574/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2774 - acc: 0.9491 - val_loss: 1.7635 - val_acc: 0.4776\n",
      "Epoch 575/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2691 - acc: 0.9491 - val_loss: 1.7525 - val_acc: 0.5373\n",
      "Epoch 576/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2740 - acc: 0.9449 - val_loss: 1.7556 - val_acc: 0.4701\n",
      "Epoch 577/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2605 - acc: 0.9491 - val_loss: 1.7619 - val_acc: 0.5149\n",
      "Epoch 578/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2623 - acc: 0.9441 - val_loss: 1.8174 - val_acc: 0.4925\n",
      "Epoch 579/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2712 - acc: 0.9449 - val_loss: 2.0177 - val_acc: 0.5075\n",
      "Epoch 580/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2850 - acc: 0.9382 - val_loss: 1.8345 - val_acc: 0.4701\n",
      "Epoch 581/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2743 - acc: 0.9432 - val_loss: 1.8957 - val_acc: 0.4552\n",
      "Epoch 582/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2602 - acc: 0.9416 - val_loss: 1.7549 - val_acc: 0.4776\n",
      "Epoch 583/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2891 - acc: 0.9382 - val_loss: 1.7098 - val_acc: 0.4776\n",
      "Epoch 584/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2646 - acc: 0.9516 - val_loss: 1.8791 - val_acc: 0.4552\n",
      "Epoch 585/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2923 - acc: 0.9290 - val_loss: 1.8210 - val_acc: 0.5000\n",
      "Epoch 586/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2941 - acc: 0.9374 - val_loss: 1.7358 - val_acc: 0.4925\n",
      "Epoch 587/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2729 - acc: 0.9391 - val_loss: 1.8292 - val_acc: 0.5299\n",
      "Epoch 588/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2735 - acc: 0.9441 - val_loss: 1.7325 - val_acc: 0.5000\n",
      "Epoch 589/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2732 - acc: 0.9357 - val_loss: 1.7171 - val_acc: 0.4925\n",
      "Epoch 590/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2619 - acc: 0.9399 - val_loss: 1.7182 - val_acc: 0.4701\n",
      "Epoch 591/600\n",
      "1198/1198 [==============================] - 0s 43us/step - loss: 0.2787 - acc: 0.9432 - val_loss: 1.8337 - val_acc: 0.5000\n",
      "Epoch 592/600\n",
      "1198/1198 [==============================] - 0s 46us/step - loss: 0.2359 - acc: 0.9599 - val_loss: 1.7895 - val_acc: 0.5149\n",
      "Epoch 593/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2484 - acc: 0.9499 - val_loss: 1.6130 - val_acc: 0.4627\n",
      "Epoch 594/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2498 - acc: 0.9599 - val_loss: 1.6898 - val_acc: 0.5075\n",
      "Epoch 595/600\n",
      "1198/1198 [==============================] - 0s 42us/step - loss: 0.2411 - acc: 0.9566 - val_loss: 1.8293 - val_acc: 0.4552\n",
      "Epoch 596/600\n",
      "1198/1198 [==============================] - 0s 45us/step - loss: 0.2543 - acc: 0.9549 - val_loss: 1.7498 - val_acc: 0.4627\n",
      "Epoch 597/600\n",
      "1198/1198 [==============================] - 0s 52us/step - loss: 0.2492 - acc: 0.9524 - val_loss: 1.7708 - val_acc: 0.4925\n",
      "Epoch 598/600\n",
      "1198/1198 [==============================] - ETA: 0s - loss: 0.2805 - acc: 0.929 - 0s 49us/step - loss: 0.2514 - acc: 0.9533 - val_loss: 1.8118 - val_acc: 0.4776\n",
      "Epoch 599/600\n",
      "1198/1198 [==============================] - 0s 48us/step - loss: 0.2661 - acc: 0.9508 - val_loss: 1.6516 - val_acc: 0.5149\n",
      "Epoch 600/600\n",
      "1198/1198 [==============================] - 0s 44us/step - loss: 0.2864 - acc: 0.9307 - val_loss: 1.8566 - val_acc: 0.4851\n",
      "87/87 [==============================] - 0s 80us/step\n",
      "Evaluation on test data: Loss - 1.752764145533244,  Acurracy - 59.770114805506566\n",
      "classification_score\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.52      0.41        23\n",
      "           1       0.78      0.62      0.70        64\n",
      "\n",
      "   micro avg       0.60      0.60      0.60        87\n",
      "   macro avg       0.56      0.57      0.55        87\n",
      "weighted avg       0.67      0.60      0.62        87\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epoch=600\n",
    "twitter_model = vizModelMetric(evaluateModelAndTestAccuracy(DenseNetwork(featureLength),train, train_y, test, test_y ,n_epoch),n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:29:35.967055Z",
     "start_time": "2019-05-20T22:29:35.199585Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'twitter_model.h5'\n",
    "twitter_model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Tw, News and FI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T21:33:34.695082Z",
     "start_time": "2019-05-20T21:33:34.692111Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:17:24.181291Z",
     "start_time": "2019-05-20T22:17:24.161372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (983, 31)\n",
      "---------------------------\n",
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (1027, 24)\n",
      "---------------------------\n",
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (1419, 770)\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "getStartEndDateShape(_ens_FI_Scaled_FE)\n",
    "getStartEndDateShape(_ens_News_NonScaled_FE)\n",
    "getStartEndDateShape(_ens_Twitter_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:16:32.369157Z",
     "start_time": "2019-05-20T22:16:32.361189Z"
    }
   },
   "outputs": [],
   "source": [
    "targtWithDateForJoin =pd.DataFrame()\n",
    "targtWithDateForJoin['date'] = _ens_FI_Scaled_FE.date.tolist()\n",
    "targtWithDateForJoin['target'] = _ens_FI_Scaled_FE.target.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:17:34.215656Z",
     "start_time": "2019-05-20T22:17:33.945405Z"
    }
   },
   "outputs": [],
   "source": [
    "_ens_FI_Scaled_FE.drop(['target'],axis=1,inplace= True)\n",
    "_ens_News_NonScaled_FE.drop(['target'],axis=1,inplace= True)\n",
    "_ens_Twitter_BERT.drop(['Target'],axis=1,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:17:48.643833Z",
     "start_time": "2019-05-20T22:17:48.613931Z"
    }
   },
   "outputs": [],
   "source": [
    "result =pd.merge( targtWithDateForJoin,_ens_FI_Scaled_FE, how='inner', on='date')\n",
    "result =pd.merge( result,_ens_News_NonScaled_FE, how='inner', on='date')\n",
    "result =pd.merge( result,_ens_Twitter_BERT, how='inner', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:17:49.449229Z",
     "start_time": "2019-05-20T22:17:49.419311Z"
    }
   },
   "outputs": [],
   "source": [
    "result.sort_values(by='date',inplace=True) \n",
    "result['target'] = result['target'].ffill()\n",
    "result.fillna(0,inplace=True)\n",
    "result.drop(['open'],axis=1,inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:45:30.410789Z",
     "start_time": "2019-05-20T22:45:30.376867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startDate 2015-05-04 00:00:00\n",
      "endDate 2019-03-28 00:00:00\n",
      "shape (838, 822)\n",
      "---------------------------\n",
      "train (782, 820) test (56, 820) train_y 782 test_y 56\n"
     ]
    }
   ],
   "source": [
    "result= createComparableDataset(result)\n",
    "\n",
    "getStartEndDateShape(result)\n",
    "train, train_y,test, test_y = prepareDataSet(result)\n",
    "print('train',train.shape,'test',test.shape,'train_y',len(train_y),'test_y',len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:17:54.317880Z",
     "start_time": "2019-05-20T22:17:54.301921Z"
    }
   },
   "outputs": [],
   "source": [
    "col_names =  ['news', 'tweeter', 'financialIndicator','target']\n",
    "YHat_train  = pd.DataFrame(columns = col_names)\n",
    "YHat_test = pd.DataFrame(columns = col_names)\n",
    "YHat_test['target'] = test_y\n",
    "YHat_train['target'] = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:18:39.532757Z",
     "start_time": "2019-05-20T22:18:39.525763Z"
    }
   },
   "outputs": [],
   "source": [
    "train_nwDf= train[[ 'lm_positive', 'lm_negative', 'lm_polarity', 'passive', 'weak',\n",
    "       'strong', 'anger', 'joy', 'suprise', 'sadness', 'disgust',\n",
    "       'anticipation', 'fear', 'trust', 'tb_polarity', 'hiv4_polarity']]\n",
    "test_nwDf= test[['lm_positive', 'lm_negative', 'lm_polarity', 'passive', 'weak',\n",
    "       'strong', 'anger', 'joy', 'suprise', 'sadness', 'disgust',\n",
    "       'anticipation', 'fear', 'trust', 'tb_polarity', 'hiv4_polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:18:44.192709Z",
     "start_time": "2019-05-20T22:18:44.185753Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fiDf= train[[ 'Volume', 'sma2_increment', 'sma2_1_increment', 'vol_increment',\n",
    "       'vol_rel_increment', 'open_incr', 'MACD', 'fft', 'absolute' ]]\n",
    "test_fiDf= test[[ 'Volume', 'sma2_increment', 'sma2_1_increment', 'vol_increment',\n",
    "       'vol_rel_increment', 'open_incr', 'MACD', 'fft', 'absolute']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:18:45.273180Z",
     "start_time": "2019-05-20T22:18:45.215337Z"
    }
   },
   "outputs": [],
   "source": [
    "train_twDf= train[[ '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624', '625', '626', '627', '628', '629', '630', '631', '632', '633', '634', '635', '636', '637', '638', '639', '640', '641', '642', '643', '644', '645', '646', '647', '648', '649', '650', '651', '652', '653', '654', '655', '656', '657', '658', '659', '660', '661', '662', '663', '664', '665', '666', '667', '668', '669', '670', '671', '672', '673', '674', '675', '676', '677', '678', '679', '680', '681', '682', '683', '684', '685', '686', '687', '688', '689', '690', '691', '692', '693', '694', '695', '696', '697', '698', '699', '700', '701', '702', '703', '704', '705', '706', '707', '708', '709', '710', '711', '712', '713', '714', '715', '716', '717', '718', '719', '720', '721', '722', '723', '724', '725', '726', '727', '728', '729', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '740', '741', '742', '743', '744', '745', '746', '747', '748', '749', '750', '751', '752', '753', '754', '755', '756', '757', '758', '759', '760', '761', '762', '763', '764', '765', '766', '767']]\n",
    "test_twDf= test[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624', '625', '626', '627', '628', '629', '630', '631', '632', '633', '634', '635', '636', '637', '638', '639', '640', '641', '642', '643', '644', '645', '646', '647', '648', '649', '650', '651', '652', '653', '654', '655', '656', '657', '658', '659', '660', '661', '662', '663', '664', '665', '666', '667', '668', '669', '670', '671', '672', '673', '674', '675', '676', '677', '678', '679', '680', '681', '682', '683', '684', '685', '686', '687', '688', '689', '690', '691', '692', '693', '694', '695', '696', '697', '698', '699', '700', '701', '702', '703', '704', '705', '706', '707', '708', '709', '710', '711', '712', '713', '714', '715', '716', '717', '718', '719', '720', '721', '722', '723', '724', '725', '726', '727', '728', '729', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '740', '741', '742', '743', '744', '745', '746', '747', '748', '749', '750', '751', '752', '753', '754', '755', '756', '757', '758', '759', '760', '761', '762', '763', '764', '765', '766', '767']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T23:17:24.370739Z",
     "start_time": "2019-05-20T23:17:24.363775Z"
    }
   },
   "outputs": [],
   "source": [
    "YHat_test['news'] = news_model.predict(test_nwDf)  \n",
    "YHat_train['news'] = news_model.predict(train_nwDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T23:17:24.662286Z",
     "start_time": "2019-05-20T23:17:24.656321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6428571428571429"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(news_model.predict(test_nwDf) ,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T23:17:25.113071Z",
     "start_time": "2019-05-20T23:17:24.878471Z"
    }
   },
   "outputs": [],
   "source": [
    "YHat_test['tweeter'] = twitter_model.predict(test_twDf) \n",
    "YHat_train['tweeter'] = twitter_model.predict(train_twDf).round() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T23:17:25.151961Z",
     "start_time": "2019-05-20T23:17:25.142948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(twitter_model.predict(test_twDf).round() ,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T23:17:26.017099Z",
     "start_time": "2019-05-20T23:17:26.010133Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_fiDf.head(2)\n",
    "scale = StandardScaler()\n",
    "train_fiDf = pd.DataFrame(scale.fit_transform(train_fiDf.values), columns=train_fi.columns)\n",
    "scale = StandardScaler()\n",
    "test_fiDf = pd.DataFrame(scale.fit_transform(test_fiDf.values), columns=test_fi.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T23:17:27.343340Z",
     "start_time": "2019-05-20T23:17:26.552449Z"
    }
   },
   "outputs": [],
   "source": [
    "YHat_test['financialIndicator'] = fi_model.predict(test_fiDf)\n",
    "YHat_train['financialIndicator'] = fi_model.predict(train_fiDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T23:17:27.646525Z",
     "start_time": "2019-05-20T23:17:27.345329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7678571428571429"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(fi_model.predict(test_fiDf),test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T23:17:28.588806Z",
     "start_time": "2019-05-20T23:17:28.584809Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T00:33:36.217778Z",
     "start_time": "2019-05-21T00:33:36.212792Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    ('logisticregression', LogisticRegression()),\n",
    "    ('randomForest', RandomForestClassifier(n_estimators=5000, max_depth=4,random_state=0)),\n",
    "    ('svc', SVC(gamma='auto')),\n",
    "]\n",
    "clf = VotingClassifier(classifiers, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T00:33:42.477705Z",
     "start_time": "2019-05-21T00:33:36.578211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('sgd', SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=6000,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "   ...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=-1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(YHat_train.drop(['target'],axis=1),train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T00:33:42.792865Z",
     "start_time": "2019-05-21T00:33:42.479673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.5714285714285714\n",
      "classification_score\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40        16\n",
      "           1       0.75      0.60      0.67        40\n",
      "\n",
      "   micro avg       0.57      0.57      0.57        56\n",
      "   macro avg       0.54      0.55      0.53        56\n",
      "weighted avg       0.63      0.57      0.59        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(YHat_test.drop(['target'],axis=1))\n",
    "print('accuracy_score',accuracy_score(y_pred, test_y))\n",
    "print('classification_score\\n',classification_report(y_pred, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "330.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
